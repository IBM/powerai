From 567f6eb4ac55acb48861e3688a437151f31ec90b Mon Sep 17 00:00:00 2001
From: Deepali Chourasia <deepali@us.ibm.com>
Date: Tue, 27 Aug 2019 10:05:57 +0000
Subject: [PATCH 1/5] Revert "open source MaskedLM and BERT models (#734)"

This reverts commit b0de88d2ced6191459cdf6a8ee0310f2dce73ad1.

Conflicts:
	pytext/builtin_task.py
	pytext/models/bert_classification_models.py
---
 pytext/builtin_task.py                      |   8 --
 pytext/models/bert_classification_models.py | 198 ----------------------------
 pytext/models/bert_regression_model.py      |  39 ------
 pytext/models/masked_lm.py                  | 168 -----------------------
 pytext/models/masking_utils.py              |  70 ----------
 pytext/task/tasks.py                        |  41 ------
 6 files changed, 524 deletions(-)
 delete mode 100644 pytext/models/bert_classification_models.py
 delete mode 100644 pytext/models/bert_regression_model.py
 delete mode 100644 pytext/models/masked_lm.py
 delete mode 100644 pytext/models/masking_utils.py

diff --git a/pytext/builtin_task.py b/pytext/builtin_task.py
index 0e7208f..d5d2a89 100644
--- a/pytext/builtin_task.py
+++ b/pytext/builtin_task.py
@@ -12,7 +12,6 @@ from pytext.task.disjoint_multitask import DisjointMultitask, NewDisjointMultita
 from pytext.task.new_task import NewTask
 from pytext.task.task import Task_Deprecated
 from pytext.task.tasks import (
-    BertPairRegressionTask,
     DocClassificationTask_Deprecated,
     DocumentClassificationTask,
     DocumentRegressionTask,
@@ -20,9 +19,6 @@ from pytext.task.tasks import (
     IntentSlotTask,
     LMTask,
     LMTask_Deprecated,
-    MaskedLMTask,
-    NewBertClassificationTask,
-    NewBertPairClassificationTask,
     PairwiseClassificationTask,
     QueryDocumentPairwiseRankingTask,
     QueryDocumentPairwiseRankingTask_Deprecated,
@@ -69,7 +65,6 @@ def add_include(path):
 def register_builtin_tasks():
     register_tasks(
         (
-            BertPairRegressionTask,
             DisjointMultitask,
             DocClassificationTask_Deprecated,
             DocumentClassificationTask,
@@ -78,9 +73,6 @@ def register_builtin_tasks():
             IntentSlotTask,
             LMTask,
             LMTask_Deprecated,
-            MaskedLMTask,
-            NewBertClassificationTask,
-            NewBertPairClassificationTask,
             NewDisjointMultitask,
             PairwiseClassificationTask,
             QueryDocumentPairwiseRankingTask,
diff --git a/pytext/models/bert_classification_models.py b/pytext/models/bert_classification_models.py
deleted file mode 100644
index 520fc56..0000000
--- a/pytext/models/bert_classification_models.py
+++ /dev/null
@@ -1,198 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
-
-from typing import Dict, List, Optional, Tuple
-
-import torch
-import torch.nn as nn
-from pytext.common.constants import Stage
-from pytext.config.component import create_loss
-from pytext.data.bert_tensorizer import BERTTensorizer
-from pytext.data.tensorizers import (
-    FloatListTensorizer,
-    LabelTensorizer,
-    NtokensTensorizer,
-    Tensorizer,
-)
-from pytext.loss import BinaryCrossEntropyLoss, MultiLabelSoftMarginLoss
-from pytext.models.decoders.mlp_decoder import MLPDecoder
-from pytext.models.model import BaseModel, ModelInputBase
-from pytext.models.module import create_module
-from pytext.models.output_layers import ClassificationOutputLayer
-from pytext.models.output_layers.doc_classification_output_layer import (
-    BinaryClassificationOutputLayer,
-    MulticlassOutputLayer,
-    MultiLabelOutputLayer,
-)
-from pytext.models.pair_classification_model import BasePairwiseModel
-from pytext.models.representations.huggingface_bert_sentence_encoder import (
-    HuggingFaceBertSentenceEncoder,
-)
-from pytext.models.representations.transformer_sentence_encoder_base import (
-    TransformerSentenceEncoderBase,
-)
-
-
-class NewBertModel(BaseModel):
-    """BERT single sentence classification."""
-
-    SUPPORT_FP16_OPTIMIZER = True
-
-    class Config(BaseModel.Config):
-        class BertModelInput(BaseModel.Config.ModelInput):
-            tokens: BERTTensorizer.Config = BERTTensorizer.Config(max_seq_len=128)
-            dense: Optional[FloatListTensorizer.Config] = None
-            labels: LabelTensorizer.Config = LabelTensorizer.Config()
-            # for metric reporter
-            num_tokens: NtokensTensorizer.Config = NtokensTensorizer.Config(
-                names=["tokens"], indexes=[2]
-            )
-
-        inputs: BertModelInput = BertModelInput()
-        encoder: TransformerSentenceEncoderBase.Config = (
-            HuggingFaceBertSentenceEncoder.Config()
-        )
-        decoder: MLPDecoder.Config = MLPDecoder.Config()
-        output_layer: ClassificationOutputLayer.Config = (
-            ClassificationOutputLayer.Config()
-        )
-
-    def arrange_model_inputs(self, tensor_dict):
-        model_inputs = (tensor_dict["tokens"],)
-        if "dense" in tensor_dict:
-            model_inputs += (tensor_dict["dense"],)
-        return model_inputs
-
-    def arrange_targets(self, tensor_dict):
-        return tensor_dict["labels"]
-
-    def forward(
-        self, encoder_inputs: Tuple[torch.Tensor, ...], *args
-    ) -> List[torch.Tensor]:
-        representation = self.encoder(encoder_inputs)[0]
-        return self.decoder(representation, *args)
-
-    def caffe2_export(self, tensorizers, tensor_dict, path, export_onnx_path=None):
-        pass
-
-    @classmethod
-    def from_config(cls, config: Config, tensorizers: Dict[str, Tensorizer]):
-        labels = tensorizers["labels"].vocab
-        vocab = tensorizers["tokens"].vocab
-        encoder = create_module(
-            config.encoder,
-            padding_idx=vocab.get_pad_index(),
-            vocab_size=vocab.__len__(),
-        )
-        dense_dim = tensorizers["dense"].dim if "dense" in tensorizers else 0
-        decoder = create_module(
-            config.decoder,
-            in_dim=encoder.representation_dim + dense_dim,
-            out_dim=len(labels),
-        )
-
-        loss = create_loss(config.output_layer.loss)
-
-        if isinstance(loss, BinaryCrossEntropyLoss):
-            output_layer_cls = BinaryClassificationOutputLayer
-        elif isinstance(loss, MultiLabelSoftMarginLoss):
-            output_layer_cls = MultiLabelOutputLayer
-        else:
-            output_layer_cls = MulticlassOutputLayer
-
-        output_layer = output_layer_cls(list(labels), loss)
-        return cls(encoder, decoder, output_layer)
-
-    def __init__(self, encoder, decoder, output_layer, stage=Stage.TRAIN) -> None:
-        super().__init__(stage=stage)
-        self.encoder = encoder
-        self.decoder = decoder
-        self.module_list = [encoder, decoder]
-        self.output_layer = output_layer
-        self.stage = stage
-        self.module_list = [encoder, decoder]
-
-
-class BertPairwiseModel(BasePairwiseModel):
-    """Bert Pairwise classification model
-
-    The model takes two sets of tokens (left and right), calculates their
-    representations separately using shared BERT encoder and passes them to
-    the decoder along with their absolute difference and elementwise product,
-    all concatenated. Used for e.g. natural language inference.
-    """
-
-    class Config(BasePairwiseModel.Config):
-        class ModelInput(ModelInputBase):
-            tokens1: BERTTensorizer.Config = BERTTensorizer.Config(
-                columns=["text1"], max_seq_len=128
-            )
-            tokens2: BERTTensorizer.Config = BERTTensorizer.Config(
-                columns=["text2"], max_seq_len=128
-            )
-            labels: LabelTensorizer.Config = LabelTensorizer.Config()
-            # for metric reporter
-            num_tokens: NtokensTensorizer.Config = NtokensTensorizer.Config(
-                names=["tokens1", "tokens2"], indexes=[2, 2]
-            )
-
-        inputs: ModelInput = ModelInput()
-        encoder: TransformerSentenceEncoderBase.Config = (
-            HuggingFaceBertSentenceEncoder.Config()
-        )
-        shared_encoder: bool = True
-
-    def __init__(
-        self, encoder1, encoder2, decoder, output_layer, encode_relations
-    ) -> None:
-        super().__init__(decoder, output_layer, encode_relations)
-        self.encoder1 = encoder1
-        self.encoder2 = encoder2
-        self.encoders = [encoder1, encoder2]
-
-    @classmethod
-    def _create_encoder(
-        cls, config: Config, tensorizers: Dict[str, Tensorizer]
-    ) -> nn.ModuleList:
-        encoder1 = create_module(
-            config.encoder,
-            padding_idx=tensorizers["tokens1"].vocab.get_pad_index(),
-            vocab_size=len(tensorizers["tokens1"].vocab),
-        )
-        if config.shared_encoder:
-            encoder2 = encoder1
-        else:
-            encoder2 = create_module(
-                config.encoder,
-                padding_idx=tensorizers["tokens2"].vocab.get_pad_index(),
-                vocab_size=len(tensorizers["tokens2"].vocab),
-            )
-        return encoder1, encoder2
-
-    @classmethod
-    def from_config(cls, config: Config, tensorizers: Dict[str, Tensorizer]):
-        encoder1, encoder2 = cls._create_encoder(config, tensorizers)
-        decoder, output_layer = cls._create_decoder(
-            config, [encoder1, encoder2], tensorizers
-        )
-        return cls(encoder1, encoder2, decoder, output_layer, config.encode_relations)
-
-    def arrange_model_inputs(self, tensor_dict):
-        return tensor_dict["tokens1"], tensor_dict["tokens2"]
-
-    def arrange_targets(self, tensor_dict):
-        return tensor_dict["labels"]
-
-    def forward(
-        self,
-        input_tuple1: Tuple[torch.Tensor, ...],
-        input_tuple2: Tuple[torch.Tensor, ...],
-    ) -> torch.Tensor:
-        encodings = [self.encoder1(input_tuple1)[0], self.encoder2(input_tuple2)[0]]
-        if self.encode_relations:
-            encodings = self._encode_relations(encodings)
-        encoding = torch.cat(encodings, -1)
-        return self.decoder(encoding)
-
-    def save_modules(self, base_path: str = "", suffix: str = ""):
-        self._save_modules(self.encoders, base_path, suffix)
diff --git a/pytext/models/bert_regression_model.py b/pytext/models/bert_regression_model.py
deleted file mode 100644
index 78a05ef..0000000
--- a/pytext/models/bert_regression_model.py
+++ /dev/null
@@ -1,39 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
-
-from typing import Dict
-
-from pytext.config import ConfigBase
-from pytext.data.bert_tensorizer import BERTTensorizer
-from pytext.data.tensorizers import NumericLabelTensorizer, Tensorizer
-from pytext.models.bert_classification_models import NewBertModel
-from pytext.models.module import create_module
-from pytext.models.output_layers import RegressionOutputLayer
-
-
-class NewBertRegressionModel(NewBertModel):
-    """BERT single sentence (or concatenated sentences) regression."""
-
-    class Config(NewBertModel.Config):
-        class InputConfig(ConfigBase):
-            tokens: BERTTensorizer.Config = BERTTensorizer.Config(
-                columns=["text1", "text2"], max_seq_len=128
-            )
-            labels: NumericLabelTensorizer.Config = NumericLabelTensorizer.Config()
-
-        inputs: InputConfig = InputConfig()
-        output_layer: RegressionOutputLayer.Config = RegressionOutputLayer.Config()
-
-    @classmethod
-    def from_config(cls, config: Config, tensorizers: Dict[str, Tensorizer]):
-        vocab = tensorizers["tokens"].vocab
-        encoder = create_module(
-            config.encoder,
-            padding_idx=vocab.get_pad_index(),
-            vocab_size=vocab.__len__(),
-        )
-        decoder = create_module(
-            config.decoder, in_dim=encoder.representation_dim, out_dim=1
-        )
-        output_layer = RegressionOutputLayer.from_config(config.output_layer)
-        return cls(encoder, decoder, output_layer)
diff --git a/pytext/models/masked_lm.py b/pytext/models/masked_lm.py
deleted file mode 100644
index b147921..0000000
--- a/pytext/models/masked_lm.py
+++ /dev/null
@@ -1,168 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
-
-from typing import Dict, List
-
-import torch
-from fairseq.modules.transformer_sentence_encoder import init_bert_params
-from pytext.common.constants import Stage
-from pytext.config import ConfigBase
-from pytext.data.bert_tensorizer import BERTTensorizer
-from pytext.data.tensorizers import Tensorizer
-from pytext.data.utils import BOS, EOS, MASK, Vocabulary
-from pytext.models.decoders.mlp_decoder import MLPDecoder
-from pytext.models.masking_utils import (
-    MaskingStrategy,
-    frequency_based_masking,
-    random_masking,
-)
-from pytext.models.model import BaseModel
-from pytext.models.module import create_module
-from pytext.models.output_layers.lm_output_layer import LMOutputLayer
-from pytext.models.representations.transformer_sentence_encoder import (
-    TransformerSentenceEncoder,
-)
-from pytext.models.representations.transformer_sentence_encoder_base import (
-    TransformerSentenceEncoderBase,
-)
-
-
-class MaskedLanguageModel(BaseModel):
-    """Masked language model for BERT style pre-training."""
-
-    SUPPORT_FP16_OPTIMIZER = True
-
-    class Config(BaseModel.Config):
-        class InputConfig(ConfigBase):
-            tokens: BERTTensorizer.Config = BERTTensorizer.Config(max_seq_len=128)
-
-        inputs: InputConfig = InputConfig()
-        encoder: TransformerSentenceEncoderBase.Config = TransformerSentenceEncoder.Config()
-        decoder: MLPDecoder.Config = MLPDecoder.Config()
-        output_layer: LMOutputLayer.Config = LMOutputLayer.Config()
-        mask_prob: float = 0.15
-        mask_bos: bool = False
-        # masking
-        masking_strategy: MaskingStrategy = MaskingStrategy.RANDOM
-        # tie weights determines whether the input embedding weights are used
-        # in the output vocabulary projection as well
-        tie_weights: bool = True
-
-    @classmethod
-    def from_config(cls, config: Config, tensorizers: Dict[str, Tensorizer]):
-        token_tensorizer = tensorizers["tokens"]
-        vocab = token_tensorizer.vocab
-
-        encoder = create_module(
-            config.encoder,
-            output_encoded_layers=True,
-            padding_idx=vocab.get_pad_index(),
-            vocab_size=vocab.__len__(),
-        )
-        decoder = create_module(
-            config.decoder, in_dim=encoder.representation_dim, out_dim=len(vocab)
-        )
-        if getattr(config.encoder, "apply_bert_init", False):
-            decoder.apply(init_bert_params)
-        # if weights are not shared then we need to ensure that the decoder
-        # params are initialized in the same was as the encoder params
-        if config.tie_weights:
-            list(decoder.mlp.modules())[-1].weight = encoder._embedding().weight
-
-        output_layer = create_module(config.output_layer, labels=vocab)
-        return cls(
-            encoder,
-            decoder,
-            output_layer,
-            token_tensorizer,
-            vocab,
-            mask_prob=config.mask_prob,
-            mask_bos=config.mask_bos,
-            masking_strategy=config.masking_strategy,
-        )
-
-    def __init__(
-        self,
-        encoder: TransformerSentenceEncoderBase,
-        decoder: MLPDecoder,
-        output_layer: LMOutputLayer,
-        token_tensorizer: BERTTensorizer,
-        vocab: Vocabulary,
-        mask_prob: float = Config.mask_prob,
-        mask_bos: float = Config.mask_bos,
-        masking_strategy: MaskingStrategy = Config.masking_strategy,
-        stage: Stage = Stage.TRAIN,
-    ) -> None:
-        super().__init__(stage=stage)
-        self.encoder = encoder
-        self.decoder = decoder
-        self.module_list = [encoder, decoder]
-        self.output_layer = output_layer
-        self.token_tensorizer = token_tensorizer
-        self.vocab = vocab
-        self.mask_prob = mask_prob
-        self.mask_bos = mask_bos
-        self.stage = stage
-        self.masking_strategy = masking_strategy
-
-        # initialize the frequency based sampling weights if these will be used
-        self.token_sampling_weights = None
-        if self.masking_strategy == MaskingStrategy.FREQUENCY:
-            self.token_sampling_weights = [x ** -0.5 for x in self.vocab.counts]
-
-    def arrange_model_inputs(self, tensor_dict):
-        tokens, *other = tensor_dict["tokens"]
-        self.mask, self.pad_mask, mask_mask, rand_mask = self._get_mask(tokens)
-        masked_tokens = self._mask_input(tokens, mask_mask, self.vocab.idx[MASK])
-        masked_tokens = self._mask_input(
-            masked_tokens, rand_mask, torch.randint_like(tokens, high=len(self.vocab))
-        )
-        return (masked_tokens,) + tuple(other)
-
-    def arrange_targets(self, tensor_dict):
-        tokens, *other = tensor_dict["tokens"]
-        masked_target = self._mask_output(tokens, self.mask)
-        # (masked targets, #predicted tokens, #input tokens)
-        return masked_target, self.mask.sum(-1), self.pad_mask.sum(-1)
-
-    def forward(self, *inputs) -> List[torch.Tensor]:
-        encoded_layers, _ = self.encoder(inputs)
-        return self.decoder(encoded_layers[-1])
-
-    def _select_tokens_to_mask(
-        self, tokens: torch.Tensor, mask_prob: float
-    ) -> torch.tensor:
-        if self.masking_strategy == MaskingStrategy.RANDOM:
-            return random_masking(tokens, mask_prob)
-        elif self.masking_strategy == MaskingStrategy.FREQUENCY:
-            return frequency_based_masking(
-                tokens, self.token_sampling_weights, mask_prob
-            )
-        else:
-            raise NotImplementedError(
-                "Specified Masking Strategy isnt currently implemented."
-            )
-
-    def _get_mask(self, tokens):
-        mask = self._select_tokens_to_mask(tokens, self.mask_prob)
-        pad_mask = (tokens != self.vocab.get_pad_index()).long()
-        mask *= pad_mask
-
-        if not self.mask_bos:
-            bos_idx = (
-                self.vocab.idx[EOS]
-                if self.token_tensorizer.use_eos_token_for_bos
-                else self.vocab.idx[BOS]
-            )
-            mask *= (tokens != bos_idx).long()
-
-        probs = torch.rand_like(tokens, dtype=torch.float)
-        rand_mask = (probs < 0.1).long() * mask
-        mask_mask = (probs >= 0.2).long() * mask
-        return mask, pad_mask, mask_mask, rand_mask
-
-    def _mask_input(self, tokens, mask, replacement):
-        return tokens * (1 - mask) + replacement * mask
-
-    def _mask_output(self, tokens, mask):
-        return tokens * mask + self.vocab.get_pad_index() * (1 - mask)
diff --git a/pytext/models/masking_utils.py b/pytext/models/masking_utils.py
deleted file mode 100644
index dde6423..0000000
--- a/pytext/models/masking_utils.py
+++ /dev/null
@@ -1,70 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
-
-from enum import Enum
-
-import numpy as np
-import torch
-
-
-class MaskingStrategy(Enum):
-    RANDOM = "random"
-    FREQUENCY = "frequency_based"
-
-
-def random_masking(tokens: torch.tensor, mask_prob: float) -> torch.Tensor:
-    """
-    Function to mask tokens randomly.
-
-    Inputs:
-        1) tokens: Tensor with token ids of shape (batch_size x seq_len)
-        2) mask_prob: Probability of masking a particular token
-
-    Outputs:
-        mask: Tensor with same shape as input tokens (batch_size x seq_len)
-            with masked  tokens represented by a 1 and everything else as 0.
-    """
-    batch_size, seq_len = tokens.size()
-    num_masked_per_seq = int(seq_len * mask_prob)
-
-    mask = np.zeros((batch_size, seq_len), dtype=np.int_)
-    mask[:, :num_masked_per_seq] = 1
-    for row in mask:
-        np.random.shuffle(row)
-    mask = torch.from_numpy(mask).to(tokens.device)
-    return mask
-
-
-def frequency_based_masking(
-    tokens: torch.tensor, token_sampling_weights: np.ndarray, mask_prob: float
-) -> torch.Tensor:
-    """
-    Function to mask tokens based on frequency.
-
-    Inputs:
-        1) tokens: Tensor with token ids of shape (batch_size x seq_len)
-        2) token_sampling_weights: numpy array with shape (batch_size x seq_len)
-            and each element representing the sampling weight assicated with
-            the corresponding token in tokens
-        3) mask_prob: Probability of masking a particular token
-
-    Outputs:
-        mask: Tensor with same shape as input tokens (batch_size x seq_len)
-            with masked  tokens represented by a 1 and everything else as 0.
-    """
-    batch_size, seq_len = tokens.size()
-    num_masked_per_batch = int(batch_size * seq_len * mask_prob)
-
-    indices = tokens.cpu().numpy().flatten()
-
-    # get the weights associated with each token
-    weights = np.take(token_sampling_weights, indices)
-
-    # sample tokens based on the computed weights
-    tokens_to_mask = np.random.choice(
-        len(weights), num_masked_per_batch, replace=False, p=weights / weights.sum()
-    )
-    mask = torch.zeros(batch_size * seq_len)
-    mask[tokens_to_mask] = 1
-    mask = mask.view(batch_size, seq_len).long().to(tokens.device)
-    return mask
diff --git a/pytext/task/tasks.py b/pytext/task/tasks.py
index c1114c7..47fa139 100644
--- a/pytext/task/tasks.py
+++ b/pytext/task/tasks.py
@@ -17,9 +17,7 @@ from pytext.data import (
     QueryDocumentPairwiseRankingDataHandler,
     SeqModelDataHandler,
 )
-from pytext.data.bert_tensorizer import BERTTensorizer
 from pytext.data.data import Data
-from pytext.data.packed_lm_data import PackedLMData
 from pytext.data.tensorizers import Tensorizer
 from pytext.exporters import DenseFeatureExporter
 from pytext.metric_reporters import (
@@ -34,16 +32,10 @@ from pytext.metric_reporters import (
     SquadMetricReporter,
     WordTaggingMetricReporter,
 )
-from pytext.metric_reporters.language_model_metric_reporter import (
-    MaskedLMMetricReporter,
-)
-from pytext.models.bert_classification_models import NewBertModel
-from pytext.models.bert_regression_model import NewBertRegressionModel
 from pytext.models.doc_model import DocModel, DocModel_Deprecated, DocRegressionModel
 from pytext.models.ensembles import BaggingDocEnsembleModel, EnsembleModel
 from pytext.models.joint_model import IntentSlotModel
 from pytext.models.language_models.lmlstm import LMLSTM, LMLSTM_Deprecated
-from pytext.models.masked_lm import MaskedLanguageModel
 from pytext.models.model import BaseModel
 from pytext.models.pair_classification_model import BasePairwiseModel, PairwiseModel
 from pytext.models.qna.bert_squad_qa import BertSquadQAModel
@@ -178,30 +170,6 @@ class DocumentRegressionTask(NewTask):
         )
 
 
-class NewBertClassificationTask(DocumentClassificationTask):
-    class Config(DocumentClassificationTask.Config):
-        model: NewBertModel.Config = NewBertModel.Config()
-
-
-class NewBertPairClassificationTask(DocumentClassificationTask):
-    class Config(DocumentClassificationTask.Config):
-        model: NewBertModel.Config = NewBertModel.Config(
-            inputs=NewBertModel.Config.BertModelInput(
-                tokens=BERTTensorizer.Config(
-                    columns=["text1", "text2"], max_seq_len=128
-                )
-            )
-        )
-        metric_reporter: ClassificationMetricReporter.Config = (
-            ClassificationMetricReporter.Config(text_column_names=["text1", "text2"])
-        )
-
-
-class BertPairRegressionTask(DocumentRegressionTask):
-    class Config(DocumentRegressionTask.Config):
-        model: NewBertRegressionModel.Config = NewBertRegressionModel.Config()
-
-
 class WordTaggingTask_Deprecated(Task_Deprecated):
     class Config(Task_Deprecated.Config):
         model: WordTaggingModel_Deprecated.Config = WordTaggingModel_Deprecated.Config()
@@ -273,15 +241,6 @@ class LMTask(NewTask):
         )
 
 
-class MaskedLMTask(NewTask):
-    class Config(NewTask.Config):
-        data: Data.Config = PackedLMData.Config()
-        model: MaskedLanguageModel.Config = MaskedLanguageModel.Config()
-        metric_reporter: MaskedLMMetricReporter.Config = (
-            MaskedLMMetricReporter.Config()
-        )
-
-
 class PairwiseClassificationTask(NewTask):
     class Config(NewTask.Config):
         model: BasePairwiseModel.Config = PairwiseModel.Config()
-- 
1.8.3.1


From a5e166eb635a3d638a100a84074bfd935e5fd3a0 Mon Sep 17 00:00:00 2001
From: Deepali Chourasia <deepali@us.ibm.com>
Date: Tue, 27 Aug 2019 10:13:59 +0000
Subject: [PATCH 2/5] Revert "open source transformer based models - data,
 tensorizers and tokenizer (#708)"

This reverts commit dd7b790d39a576fd004bee0f64c080ecf2d49439.

Conflicts:
	pytext/data/bert_tensorizer.py
	pytext/data/packed_lm_data.py
	pytext/data/test/tensorizers_test.py
	pytext/data/xlm_tensorizer.py
---
 pytext/data/bert_tensorizer.py         |   94 ---
 pytext/data/packed_lm_data.py          |  154 -----
 pytext/data/test/data/wordpiece_1k.txt | 1000 --------------------------------
 pytext/data/test/tensorizers_test.py   |  101 ----
 pytext/data/tokenizers/__init__.py     |    4 +-
 pytext/data/tokenizers/tokenizer.py    |   81 +--
 pytext/data/xlm_dictionary.py          |  239 --------
 pytext/data/xlm_tensorizer.py          |  279 ---------
 8 files changed, 4 insertions(+), 1948 deletions(-)
 delete mode 100644 pytext/data/bert_tensorizer.py
 delete mode 100644 pytext/data/packed_lm_data.py
 delete mode 100644 pytext/data/test/data/wordpiece_1k.txt
 delete mode 100644 pytext/data/xlm_dictionary.py
 delete mode 100644 pytext/data/xlm_tensorizer.py

diff --git a/pytext/data/bert_tensorizer.py b/pytext/data/bert_tensorizer.py
deleted file mode 100644
index 04eeea3..0000000
--- a/pytext/data/bert_tensorizer.py
+++ /dev/null
@@ -1,94 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
-
-import itertools
-from typing import List
-
-from fairseq.data.legacy.masked_lm_dictionary import BertDictionary
-from pytext.config.component import ComponentType, create_component
-from pytext.data.tensorizers import TokenTensorizer
-from pytext.data.tokenizers import Tokenizer, WordPieceTokenizer
-from pytext.data.utils import BOS, EOS, MASK, PAD, UNK, Vocabulary, pad_and_tensorize
-
-
-class BERTTensorizer(TokenTensorizer):
-    """
-    Tensorizer for BERT tasks.  Works for single sentence, sentence pair, triples etc.
-    """
-
-    __EXPANSIBLE__ = True
-
-    class Config(TokenTensorizer.Config):
-        #: The tokenizer to use to split input text into tokens.
-        columns: List[str] = ["text"]
-        tokenizer: Tokenizer.Config = WordPieceTokenizer.Config()
-        add_bos_token: bool = False
-        add_eos_token: bool = True
-        bos_token: str = "[CLS]"
-        eos_token: str = "[SEP]"
-        pad_token: str = "[PAD]"
-        unk_token: str = "[UNK]"
-        mask_token: str = "[MASK]"
-        vocab_file: str = WordPieceTokenizer.Config().wordpiece_vocab_path
-
-    @classmethod
-    def from_config(cls, config: Config, **kwargs):
-        tokenizer = create_component(ComponentType.TOKENIZER, config.tokenizer)
-        replacements = {
-            config.unk_token: UNK,
-            config.pad_token: PAD,
-            config.bos_token: BOS,
-            config.eos_token: EOS,
-            config.mask_token: MASK,
-        }
-        if isinstance(tokenizer, WordPieceTokenizer):
-            vocab = Vocabulary(
-                [token for token, _ in tokenizer.vocab.items()],
-                replacements=replacements,
-            )
-        else:
-            dictionary = BertDictionary.load(config.vocab_file)
-            vocab = Vocabulary(
-                dictionary.symbols, dictionary.count, replacements=replacements
-            )
-        return cls(
-            columns=config.columns,
-            tokenizer=tokenizer,
-            add_bos_token=config.add_bos_token,
-            add_eos_token=config.add_eos_token,
-            use_eos_token_for_bos=config.use_eos_token_for_bos,
-            max_seq_len=config.max_seq_len,
-            vocab=vocab,
-            **kwargs,
-        )
-
-    def __init__(self, columns, **kwargs):
-        super().__init__(text_column=None, **kwargs)
-        self.columns = columns
-        # Manually initialize column_schema since we are sending None to TokenTensorizer
-
-    @property
-    def column_schema(self):
-        return [(column, str) for column in self.columns]
-
-    def numberize(self, row):
-        """Tokenize, look up in vocabulary."""
-        sentences = [self._lookup_tokens(row[column])[0] for column in self.columns]
-        sentences[0] = [self.vocab.idx[BOS]] + sentences[0]
-        seq_lens = (len(sentence) for sentence in sentences)
-        segment_labels = ([i] * seq_len for i, seq_len in enumerate(seq_lens))
-        tokens = list(itertools.chain(*sentences))
-        segment_labels = list(itertools.chain(*segment_labels))
-        seq_len = len(tokens)
-        # tokens, segment_label, seq_len
-        return tokens, segment_labels, seq_len
-
-    def sort_key(self, row):
-        return row[2]
-
-    def tensorize(self, batch):
-        tokens, segment_labels, seq_lens = zip(*batch)
-        tokens = pad_and_tensorize(tokens, self.vocab.get_pad_index())
-        pad_mask = (tokens != self.vocab.get_pad_index()).long()
-        segment_labels = pad_and_tensorize(segment_labels, self.vocab.get_pad_index())
-        return tokens, pad_mask, segment_labels
diff --git a/pytext/data/packed_lm_data.py b/pytext/data/packed_lm_data.py
deleted file mode 100644
index 9090401..0000000
--- a/pytext/data/packed_lm_data.py
+++ /dev/null
@@ -1,154 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
-
-from typing import Dict, List, Optional, Type
-
-from pytext.common.constants import Stage
-from pytext.data import Batcher, Data
-from pytext.data.bert_tensorizer import BERTTensorizer
-from pytext.data.data import RowData
-from pytext.data.sources import DataSource
-from pytext.data.tensorizers import Tensorizer, TokenTensorizer
-from pytext.data.xlm_tensorizer import XLMTensorizer
-
-
-class PackedLMData(Data):
-    """
-    Special purpose Data object which assumes a single text tensorizer.  Packs
-    tokens into a square batch with no padding.  Used for LM training. The object
-    also takes in an optional language argument which is used for cross-lingual
-    LM training.
-    """
-
-    __EXPANSIBLE__ = True
-
-    class Config(Data.Config):
-        max_seq_len: int = 128
-
-    @classmethod
-    def from_config(
-        cls,
-        config: Config,
-        schema: Dict[str, Type],
-        tensorizers: Dict[str, Tensorizer],
-        language: Optional[str] = None,
-        rank: int = 0,
-        world_size: int = 1,
-        init_tensorizers: Optional[bool] = True,
-    ):
-        return super(PackedLMData, cls).from_config(
-            config,
-            schema,
-            tensorizers,
-            rank,
-            world_size,
-            language=language,
-            max_seq_len=config.max_seq_len,
-            init_tensorizers=init_tensorizers,
-        )
-
-    def __init__(
-        self,
-        data_source: DataSource,
-        tensorizers: Dict[str, Tensorizer],
-        batcher: Batcher = None,
-        max_seq_len: int = Config.max_seq_len,
-        sort_key: Optional[str] = None,
-        # language is used in cross-lingual LM training
-        language: Optional[str] = None,
-        in_memory: Optional[bool] = False,
-        init_tensorizers: Optional[bool] = True,
-    ):
-        super().__init__(
-            data_source, tensorizers, batcher, sort_key, in_memory, init_tensorizers
-        )
-        assert len(list(self.tensorizers.items())) == 1
-        self.tensorizer_name, self.tensorizer = list(self.tensorizers.items())[0]
-        self.remainder: Dict[str, List[int]] = {"tokens": [], "segment_labels": []}
-        self.max_seq_len = max_seq_len
-        self.language = language
-        self.batch = {Stage.TRAIN: None, Stage.EVAL: None, Stage.TEST: None}
-
-    def _parse_row(self, row):
-        """
-        The output of numberization has different number of elements depending on
-        the tensorizer used. For example: positions tensor is only output by the
-        XLMTensorizer. This function unpacks the elements according to the
-        specific tensorizer used.
-        Additionally, since we are packing tokens into fixed size
-        blocks, we don't need to use the positions vector output by the call to
-        numberize. We will simply create this in `_format_output_row`.
-        """
-        numberized_row = self.tensorizer.numberize(row)
-        if isinstance(self.tensorizer, XLMTensorizer):
-            tokens, seq_len, segment_labels, _ = numberized_row
-        elif isinstance(self.tensorizer, BERTTensorizer):
-            tokens, segment_labels, seq_len = numberized_row
-        elif isinstance(self.tensorizer, TokenTensorizer):
-            tokens, seq_len, _ = numberized_row
-            segment_labels = []
-        else:
-            raise NotImplementedError(
-                "PackedLMData only supports XLMTensorizer, BERTTensorizer and "
-                "TokenTensorizer."
-            )
-        return tokens, segment_labels, seq_len
-
-    def _format_output_row(self, tokens, segment_labels, seq_len):
-        """
-        The tensorize function for different tensorizers takes in different
-        number of inputs which may be arranged differently. This function formats
-        the output dict to conform to the expectations of the tensorizer.
-        In case of the XLMTensorizer, we also need to create a new positions list
-        which goes from 0 to seq_len.
-        """
-        if isinstance(self.tensorizer, XLMTensorizer):
-            positions = [index for index in range(seq_len)]
-            return {self.tensorizer_name: (tokens, seq_len, segment_labels, positions)}
-        elif isinstance(self.tensorizer, BERTTensorizer):
-            return {self.tensorizer_name: (tokens, segment_labels, seq_len)}
-        elif isinstance(self.tensorizer, TokenTensorizer):
-            # dummy token_ranges
-            return {self.tensorizer_name: (tokens, seq_len, [(-1, -1)] * seq_len)}
-        else:
-            raise NotImplementedError(
-                "PackedLMData only supports BERTTensorizer and TokenTensorizer."
-            )
-
-    def _yield_and_reset(self):
-        packed_tokens = list(self.remainder["tokens"])
-        packed_segments = list(self.remainder["segment_labels"])
-        self.remainder: Dict[str, List[int]] = {"tokens": [], "segment_labels": []}
-        return RowData(
-            {},  # packed LM data doesn't respect data cardinality
-            self._format_output_row(packed_tokens, packed_segments, len(packed_tokens)),
-        )
-
-    def numberize_rows(self, rows):
-        """
-        This function does the actual packing. It processes rows until we obtain
-        a block of data with length = max_seq_len.
-        """
-        for row in rows:
-
-            # if the packedLM object has a language member then a cross-lingual
-            # LM is being trained using monolingual data.
-            # Add this language to the row since the underlying
-            # tensorizer needs this to generate language embeddings (used as
-            # segment_labels below)
-            if self.language:
-                row["language"] = self.language
-
-            tokens, segment_labels, seq_len = self._parse_row(row)
-            remaining = self.max_seq_len - len(self.remainder["tokens"]) - 1
-            while remaining < len(tokens):
-                self.remainder["tokens"].extend(tokens[:remaining])
-                self.remainder["segment_labels"].extend(segment_labels[:remaining])
-                tokens = tokens[remaining:]
-                segment_labels = segment_labels[remaining:]
-                yield self._yield_and_reset()
-                remaining = self.max_seq_len - 1
-            self.remainder["tokens"].extend(tokens)
-            self.remainder["segment_labels"].extend(segment_labels)
-        if len(self.remainder["tokens"]):
-            yield self._yield_and_reset()
diff --git a/pytext/data/test/data/wordpiece_1k.txt b/pytext/data/test/data/wordpiece_1k.txt
deleted file mode 100644
index 3f9f713..0000000
--- a/pytext/data/test/data/wordpiece_1k.txt
+++ /dev/null
@@ -1,1000 +0,0 @@
-[PAD]
-[unused1]
-[unused2]
-[unused3]
-[unused4]
-[unused5]
-[unused6]
-[unused7]
-[unused8]
-[unused9]
-[unused10]
-[unused11]
-[unused12]
-[unused13]
-[unused14]
-[unused15]
-[unused16]
-[unused17]
-[unused18]
-[unused19]
-[unused20]
-[unused21]
-[unused22]
-[unused23]
-[unused24]
-[unused25]
-[unused26]
-[unused27]
-[unused28]
-[unused29]
-[unused30]
-[unused31]
-[unused32]
-[unused33]
-[unused34]
-[unused35]
-[unused36]
-[unused37]
-[unused38]
-[unused39]
-[unused40]
-[unused41]
-[unused42]
-[unused43]
-[unused44]
-[unused45]
-[unused46]
-[unused47]
-[unused48]
-[unused49]
-[unused50]
-[unused51]
-[unused52]
-[unused53]
-[unused54]
-[unused55]
-[unused56]
-[unused57]
-[unused58]
-[unused59]
-[unused60]
-[unused61]
-[unused62]
-[unused63]
-[unused64]
-[unused65]
-[unused66]
-[unused67]
-[unused68]
-[unused69]
-[unused70]
-[unused71]
-[unused72]
-[unused73]
-[unused74]
-[unused75]
-[unused76]
-[unused77]
-[unused78]
-[unused79]
-[unused80]
-[unused81]
-[unused82]
-[unused83]
-[unused84]
-[unused85]
-[unused86]
-[unused87]
-[unused88]
-[unused89]
-[unused90]
-[unused91]
-[unused92]
-[unused93]
-[unused94]
-[unused95]
-[unused96]
-[unused97]
-[unused98]
-[unused99]
-[UNK]
-[CLS]
-[SEP]
-[MASK]
-[unused100]
-[unused101]
-!
-"
-#
-$
-%
-&
-'
-(
-)
-*
-+
-,
--
-.
-/
-0
-1
-2
-3
-4
-5
-6
-7
-8
-9
-:
-;
-<
-=
->
-?
-@
-A
-B
-C
-D
-E
-F
-G
-H
-I
-J
-K
-L
-M
-N
-O
-P
-Q
-R
-S
-T
-U
-V
-W
-X
-Y
-Z
-[
-\
-]
-^
-_
-`
-a
-b
-c
-d
-e
-f
-g
-h
-i
-j
-k
-l
-m
-n
-o
-p
-q
-r
-s
-t
-u
-v
-w
-x
-y
-z
-{
-|
-}
-~
-，
-－
-／
-：
-the
-of
-and
-to
-in
-was
-The
-is
-for
-as
-on
-with
-that
-##s
-his
-by
-he
-at
-from
-it
-her
-He
-had
-an
-were
-you
-be
-In
-she
-are
-but
-which
-It
-not
-or
-have
-my
-him
-one
-this
-me
-has
-also
-up
-their
-first
-out
-who
-been
-they
-She
-into
-all
-would
-its
-##ing
-time
-two
-##a
-##e
-said
-about
-when
-over
-more
-other
-can
-after
-back
-them
-then
-##ed
-there
-like
-so
-only
-##n
-could
-##d
-##i
-##y
-what
-no
-##o
-where
-This
-made
-than
-if
-You
-##ly
-through
-we
-before
-##r
-just
-some
-##er
-years
-do
-New
-##t
-down
-between
-new
-now
-will
-three
-most
-On
-around
-year
-used
-such
-being
-well
-during
-They
-know
-against
-under
-later
-did
-part
-known
-off
-while
-His
-re
-...
-##l
-people
-until
-way
-American
-didn
-University
-your
-both
-many
-get
-United
-became
-head
-There
-second
-As
-work
-any
-But
-still
-again
-born
-even
-eyes
-After
-including
-de
-took
-And
-long
-team
-season
-family
-see
-right
-same
-called
-name
-because
-film
-don
-10
-found
-much
-school
-##es
-going
-won
-place
-away
-We
-day
-left
-John
-000
-hand
-since
-World
-these
-how
-make
-number
-each
-life
-area
-man
-four
-go
-No
-here
-very
-National
-##m
-played
-released
-never
-began
-States
-album
-home
-last
-too
-held
-several
-May
-own
-##on
-take
-end
-School
-##h
-ll
-series
-What
-want
-use
-another
-city
-When
-2010
-side
-At
-may
-That
-came
-face
-June
-think
-game
-those
-high
-March
-early
-September
-##al
-2011
-looked
-July
-state
-small
-thought
-went
-January
-October
-##u
-based
-August
-##us
-world
-good
-April
-York
-us
-12
-2012
-2008
-For
-2009
-group
-along
-few
-South
-little
-##k
-following
-November
-something
-2013
-December
-set
-2007
-old
-2006
-2014
-located
-##an
-music
-County
-City
-former
-##in
-room
-ve
-next
-All
-##man
-got
-father
-house
-##g
-body
-15
-20
-18
-started
-If
-2015
-town
-our
-line
-War
-large
-population
-named
-British
-company
-member
-five
-My
-single
-##en
-age
-State
-moved
-February
-11
-Her
-should
-century
-government
-built
-come
-best
-show
-However
-within
-look
-men
-door
-without
-need
-wasn
-2016
-water
-One
-system
-knew
-every
-died
-League
-turned
-asked
-North
-St
-wanted
-building
-received
-song
-served
-though
-felt
-##ia
-station
-band
-##ers
-local
-public
-himself
-different
-death
-say
-##1
-30
-##2
-2005
-16
-night
-behind
-children
-English
-members
-near
-saw
-together
-son
-14
-voice
-village
-13
-hands
-help
-##3
-due
-French
-London
-top
-told
-open
-published
-third
-2017
-play
-across
-During
-put
-final
-often
-include
-25
-##le
-main
-having
-2004
-once
-ever
-let
-book
-led
-gave
-late
-front
-find
-club
-##4
-German
-included
-species
-College
-form
-opened
-mother
-women
-enough
-West
-must
-2000
-power
-really
-17
-making
-half
-##6
-order
-might
-##is
-given
-million
-times
-days
-point
-full
-service
-With
-km
-major
-##7
-original
-become
-seen
-II
-north
-six
-##te
-love
-##0
-national
-International
-##5
-24
-So
-District
-lost
-run
-couldn
-career
-always
-##9
-2003
-##th
-country
-##z
-House
-air
-tell
-south
-worked
-woman
-player
-##A
-almost
-war
-River
-##ic
-married
-continued
-Then
-James
-close
-black
-short
-##8
-##na
-using
-history
-returned
-light
-car
-##ra
-sure
-William
-things
-General
-##ry
-2002
-better
-support
-100
-among
-From
-feet
-King
-anything
-21
-19
-established
-district
-2001
-feel
-great
-##ton
-level
-Cup
-These
-written
-games
-others
-already
-title
-story
-##p
-law
-thing
-US
-record
-role
-however
-By
-students
-England
-white
-control
-least
-inside
-land
-##C
-22
-give
-community
-hard
-##ie
-non
-##c
-produced
-George
-round
-period
-Park
-business
-various
-##ne
-does
-present
-wife
-far
-taken
-per
-reached
-David
-able
-version
-working
-young
-live
-created
-joined
-East
-living
-appeared
-case
-High
-done
-23
-important
-President
-Award
-France
-position
-office
-looking
-total
-general
-class
-To
-production
-##S
-football
-party
-brother
-keep
-mind
-free
-Street
-hair
-announced
-development
-either
-nothing
-moment
-Church
-followed
-wrote
-why
-India
-San
-election
-1999
-lead
-How
-##ch
-##rs
-words
-European
-course
-considered
-America
-arms
-Army
-political
-##la
-28
-26
-west
-east
-ground
-further
-church
-less
-site
-First
-Not
-Australia
-toward
-California
-##ness
-described
-works
-An
-Council
-heart
-past
-military
-27
-##or
-heard
-field
-human
-soon
-founded
-1998
-playing
-trying
-##x
-##ist
-##ta
-television
-mouth
-although
-taking
-win
-fire
-Division
-##ity
-Party
-Royal
-program
-Some
-Don
-Association
-According
-tried
-TV
-Paul
-outside
-daughter
-Best
-While
-someone
-match
-recorded
-Canada
-closed
-region
-Air
-above
-months
-elected
-##da
-##ian
-road
-##ar
-brought
-move
-1997
-leave
-##um
-Thomas
-1996
-am
-low
-Robert
-formed
-person
-services
-points
-Mr
-miles
-##b
-stop
-rest
-doing
-needed
-international
-release
-floor
-start
-sound
-call
-killed
-real
-dark
-research
-finished
-language
-Michael
-professional
-change
-sent
-50
-upon
-29
-track
-hit
-event
-2018
-term
-example
-Germany
-similar
-return
-##ism
-fact
-pulled
-stood
-says
-ran
-information
-yet
-result
-developed
-girl
-##re
-God
-1995
-areas
-signed
-decided
-##ment
-Company
-seemed
-##el
-co
-turn
-race
-common
-video
-Charles
-Indian
-##ation
-blood
-art
-red
-##able
-added
-rather
-1994
diff --git a/pytext/data/test/tensorizers_test.py b/pytext/data/test/tensorizers_test.py
index c62f98f..482f2fb 100644
--- a/pytext/data/test/tensorizers_test.py
+++ b/pytext/data/test/tensorizers_test.py
@@ -6,7 +6,6 @@ from typing import List
 
 import numpy as np
 import torch
-from pytext.data.bert_tensorizer import BERTTensorizer
 from pytext.data.sources import SquadDataSource
 from pytext.data.sources.data_source import Gazetteer, SafeFileWrapper, load_float_list
 from pytext.data.sources.tsv import SessionTSVDataSource, TSVDataSource
@@ -26,7 +25,6 @@ from pytext.data.tensorizers import (
     VocabFileConfig,
     initialize_tensorizers,
 )
-from pytext.data.tokenizers import Tokenizer, WordPieceTokenizer
 from pytext.utils.test import import_tests_module
 
 
@@ -566,105 +564,6 @@ class TensorizersTest(unittest.TestCase):
             actions = nbrz.numberize(row)
             self.assertEqual(expected, actions)
 
-
-class BERTTensorizerTest(unittest.TestCase):
-    def test_bert_tensorizer(self):
-        sentence = "<SOS>  Focus Driving School Mulungushi bus station along Kasuba road, wamkopeka building.  Ndola,  Zambia."
-        # expected result was obtained offline by running BertModelDataHandler
-        expected = [
-            101,
-            133,
-            278,
-            217,
-            135,
-            175,
-            287,
-            766,
-            462,
-            100,
-            379,
-            182,
-            459,
-            334,
-            459,
-            280,
-            504,
-            462,
-            425,
-            283,
-            171,
-            462,
-            567,
-            474,
-            180,
-            262,
-            217,
-            459,
-            931,
-            262,
-            913,
-            117,
-            192,
-            262,
-            407,
-            478,
-            287,
-            744,
-            263,
-            478,
-            262,
-            560,
-            119,
-            183,
-            282,
-            287,
-            843,
-            117,
-            195,
-            262,
-            407,
-            931,
-            566,
-            119,
-            102,
-        ]
-        row = {"text": sentence}
-        tensorizer = BERTTensorizer.from_config(
-            BERTTensorizer.Config(
-                tokenizer=WordPieceTokenizer.Config(
-                    wordpiece_vocab_path="pytext/data/test/data/wordpiece_1k.txt"
-                )
-            )
-        )
-        tokens, segment_label, seq_len = tensorizer.numberize(row)
-        self.assertEqual(tokens, expected)
-        self.assertEqual(seq_len, len(expected))
-        self.assertEqual(segment_label, [0] * len(expected))
-
-        tokens, pad_mask, segment_labels = tensorizer.tensorize(
-            [(tokens, segment_label, seq_len)]
-        )
-        self.assertEqual(pad_mask[0].tolist(), [1] * len(expected))
-
-    def test_bert_pair_tensorizer(self):
-        sentences = ["Focus", "Driving School"]
-        expected_tokens = [101, 175, 287, 766, 462, 102, 100, 379, 102]
-        expected_segment_labels = [0, 0, 0, 0, 0, 0, 1, 1, 1]
-        row = {"text1": sentences[0], "text2": sentences[1]}
-        tensorizer = BERTTensorizer.from_config(
-            BERTTensorizer.Config(
-                columns=["text1", "text2"],
-                tokenizer=WordPieceTokenizer.Config(
-                    wordpiece_vocab_path="pytext/data/test/data/wordpiece_1k.txt"
-                ),
-            )
-        )
-        tokens, segment_labels, seq_len = tensorizer.numberize(row)
-        self.assertEqual(tokens, expected_tokens)
-        self.assertEqual(segment_labels, expected_segment_labels)
-        self.assertEqual(seq_len, len(expected_tokens))
-
-
 class SquadForBERTTensorizerTest(unittest.TestCase):
     def test_squad_tensorizer(self):
         source = SquadDataSource.from_config(
diff --git a/pytext/data/tokenizers/__init__.py b/pytext/data/tokenizers/__init__.py
index afb5fcb..dcdc340 100644
--- a/pytext/data/tokenizers/__init__.py
+++ b/pytext/data/tokenizers/__init__.py
@@ -1,7 +1,7 @@
 #!/usr/bin/env python3
 # Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
 
-from .tokenizer import DoNothingTokenizer, Token, Tokenizer, WordPieceTokenizer
+from .tokenizer import DoNothingTokenizer, Token, Tokenizer
 
 
-__all__ = ["Token", "Tokenizer", "DoNothingTokenizer", "WordPieceTokenizer"]
+__all__ = ["Token", "Tokenizer", "DoNothingTokenizer"]
diff --git a/pytext/data/tokenizers/tokenizer.py b/pytext/data/tokenizers/tokenizer.py
index 5eec33a..afa7ccb 100644
--- a/pytext/data/tokenizers/tokenizer.py
+++ b/pytext/data/tokenizers/tokenizer.py
@@ -4,13 +4,7 @@
 import re
 from typing import List, NamedTuple
 
-from pytext.config import ConfigBase
-from pytext.config.component import Component, ComponentType, create_component
-from pytorch_pretrained_bert.tokenization import (
-    BasicTokenizer,
-    WordpieceTokenizer,
-    load_vocab,
-)
+from pytext.config.component import Component, ComponentType
 
 
 class Token(NamedTuple):
@@ -58,7 +52,7 @@ class Tokenizer(Component):
 class DoNothingTokenizer(Tokenizer):
     """
     Tokenizer that takes a list of strings and converts to a list of Tokens.
-    Useful in cases where tokenizer is run before-hand
+    Used for Fluent2 integration, where tokenizer is run before-hand
     """
 
     class Config(Component.Config):
@@ -74,74 +68,3 @@ class DoNothingTokenizer(Tokenizer):
     def tokenize(self, input: List[str]) -> List[Token]:
         tokens = [Token(token_text, -1, -1) for token_text in input if token_text]
         return tokens
-
-
-class BERTInitialTokenizer(Tokenizer):
-    """
-    Basic initial tokenization for BERT.  This is run prior to word piece, does
-    white space tokenization in addition to lower-casing and accent removal
-    if specified.
-    """
-
-    class Config(Tokenizer.Config):
-        """Config for this class."""
-
-    @classmethod
-    def from_config(cls, config: Config):
-        basic_tokenizer = BasicTokenizer(do_lower_case=config.lowercase)
-        return cls(basic_tokenizer)
-
-    def __init__(self, basic_tokenizer) -> None:
-        self.tokenizer = basic_tokenizer
-
-    def tokenize(self, text):
-        """Tokenizes a piece of text."""
-        if self.tokenizer.do_lower_case:
-            text = self.tokenizer._run_strip_accents(text.lower())
-        tokens = self.tokenizer.tokenize(text)
-        end = 0
-        result = []
-        for token in tokens:
-            start = text.find(token, end)
-            if start == -1:  # safety check, this should not happen
-                start = end
-            end = start + len(token)
-            result.append(Token(token, start, end))
-        return result
-
-
-class WordPieceTokenizer(Tokenizer):
-    """Word piece tokenizer for BERT models."""
-
-    class Config(ConfigBase):
-        basic_tokenizer: BERTInitialTokenizer.Config = BERTInitialTokenizer.Config()
-        wordpiece_vocab_path: str = "/mnt/vol/nlp_technologies/bert/uncased_L-12_H-768_A-12/vocab.txt"
-
-    def __init__(self, wordpiece_vocab, basic_tokenizer, wordpiece_tokenizer) -> None:
-        self.vocab = wordpiece_vocab
-        self.basic_tokenizer = basic_tokenizer
-        self.wordpiece_tokenizer = wordpiece_tokenizer
-
-    @classmethod
-    def from_config(cls, config: Config):
-        basic_tokenizer = create_component(
-            ComponentType.TOKENIZER, config.basic_tokenizer
-        )
-        vocab = load_vocab(config.wordpiece_vocab_path)
-        wordpiece_tokenizer = WordpieceTokenizer(vocab=vocab)
-        return cls(vocab, basic_tokenizer, wordpiece_tokenizer)
-
-    def tokenize(self, input_str: str) -> List[Token]:
-        tokens = []
-        for token in self.basic_tokenizer.tokenize(input_str):
-            start = token.start
-            for sub_token in self.wordpiece_tokenizer.tokenize(token.value):
-                piece_len = (
-                    len(sub_token)
-                    if not sub_token.startswith("##")
-                    else (len(sub_token) - 2)  # account for ##
-                )
-                end = start + piece_len
-                tokens.append(Token(sub_token, start, end))
-                start = end
-        return [token for token in tokens if token.value]
diff --git a/pytext/data/xlm_dictionary.py b/pytext/data/xlm_dictionary.py
deleted file mode 100644
index 53cad13..0000000
--- a/pytext/data/xlm_dictionary.py
+++ /dev/null
@@ -1,239 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
-
-
-import os
-from logging import getLogger
-
-import numpy as np
-import torch
-
-
-logger = getLogger()
-
-
-BOS_WORD = "<s>"
-EOS_WORD = "</s>"
-PAD_WORD = "<pad>"
-UNK_WORD = "<unk>"
-
-SPECIAL_WORD = "<special%i>"
-SPECIAL_WORDS = 10
-
-SEP_WORD = SPECIAL_WORD % 0
-MASK_WORD = SPECIAL_WORD % 1
-
-
-class Dictionary(object):
-    def __init__(self, id2word, word2id, counts):
-        assert len(id2word) == len(word2id) == len(counts)
-        self.id2word = id2word
-        self.word2id = word2id
-        self.counts = counts
-        self.bos_index = word2id[BOS_WORD]
-        self.eos_index = word2id[EOS_WORD]
-        self.pad_index = word2id[PAD_WORD]
-        self.unk_index = word2id[UNK_WORD]
-        self.check_valid()
-
-    def __len__(self):
-        """
-        Returns the number of words in the dictionary.
-        """
-        return len(self.id2word)
-
-    def __getitem__(self, i):
-        """
-        Returns the word of the specified index.
-        """
-        return self.id2word[i]
-
-    def __contains__(self, w):
-        """
-        Returns whether a word is in the dictionary.
-        """
-        return w in self.word2id
-
-    def __eq__(self, y):
-        """
-        Compare this dictionary with another one.
-        """
-        self.check_valid()
-        y.check_valid()
-        if len(self.id2word) != len(y):
-            return False
-        return all(self.id2word[i] == y[i] for i in range(len(y)))
-
-    def check_valid(self):
-        """
-        Check that the dictionary is valid.
-        """
-        assert self.bos_index == 0
-        assert self.eos_index == 1
-        assert self.pad_index == 2
-        assert self.unk_index == 3
-        assert all(
-            self.id2word[4 + i] == SPECIAL_WORD % i for i in range(SPECIAL_WORDS)
-        )
-        assert len(self.id2word) == len(self.word2id) == len(self.counts)
-        assert set(self.word2id.keys()) == set(self.counts.keys())
-        for i in range(len(self.id2word)):
-            assert self.word2id[self.id2word[i]] == i
-        last_count = 1e18
-        for i in range(4 + SPECIAL_WORDS, len(self.id2word) - 1):
-            count = self.counts[self.id2word[i]]
-            assert count <= last_count
-            last_count = count
-
-    def index(self, word, no_unk=False):
-        """
-        Returns the index of the specified word.
-        """
-        if no_unk:
-            return self.word2id[word]
-        else:
-            return self.word2id.get(word, self.unk_index)
-
-    def max_vocab(self, max_vocab):
-        """
-        Limit the vocabulary size.
-        """
-        assert max_vocab >= 1
-        init_size = len(self)
-        self.id2word = {k: v for k, v in self.id2word.items() if k < max_vocab}
-        self.word2id = {v: k for k, v in self.id2word.items()}
-        self.counts = {k: v for k, v in self.counts.items() if k in self.word2id}
-        self.check_valid()
-        logger.info(
-            "Maximum vocabulary size: %i. Dictionary size: %i -> %i (removed %i words)."
-            % (max_vocab, init_size, len(self), init_size - len(self))
-        )
-
-    def min_count(self, min_count):
-        """
-        Threshold on the word frequency counts.
-        """
-        assert min_count >= 0
-        init_size = len(self)
-        self.id2word = {
-            k: v
-            for k, v in self.id2word.items()
-            if self.counts[self.id2word[k]] >= min_count or k < 4 + SPECIAL_WORDS
-        }
-        self.word2id = {v: k for k, v in self.id2word.items()}
-        self.counts = {k: v for k, v in self.counts.items() if k in self.word2id}
-        self.check_valid()
-        logger.info(
-            "Minimum frequency count: %i. Dictionary size: %i -> %i (removed %i words)."
-            % (min_count, init_size, len(self), init_size - len(self))
-        )
-
-    @staticmethod
-    def read_vocab(vocab_path):
-        """
-        Create a dictionary from a vocabulary file.
-        """
-        skipped = 0
-        assert os.path.isfile(vocab_path), vocab_path
-        word2id = {BOS_WORD: 0, EOS_WORD: 1, PAD_WORD: 2, UNK_WORD: 3}
-        for i in range(SPECIAL_WORDS):
-            word2id[SPECIAL_WORD % i] = 4 + i
-        counts = {k: 0 for k in word2id.keys()}
-        f = open(vocab_path, "r", encoding="utf-8")
-        for i, line in enumerate(f):
-            if "\u2028" in line:
-                skipped += 1
-                continue
-            line = line.rstrip().split()
-            if len(line) != 2:
-                skipped += 1
-                continue
-            assert len(line) == 2, (i, line)
-            # assert line[0] not in word2id and line[1].isdigit(), (i, line)
-            assert line[1].isdigit(), (i, line)
-            if line[0] in word2id:
-                skipped += 1
-                print("%s already in vocab" % line[0])
-                continue
-            if not line[1].isdigit():
-                skipped += 1
-                print("Empty word at line %s with count %s" % (i, line))
-                continue
-            # shift because of extra words
-            word2id[line[0]] = 4 + SPECIAL_WORDS + i - skipped
-            counts[line[0]] = int(line[1])
-        f.close()
-        id2word = {v: k for k, v in word2id.items()}
-        dico = Dictionary(id2word, word2id, counts)
-        logger.info("Read %i words from the vocabulary file." % len(dico))
-        if skipped > 0:
-            logger.warning("Skipped %i empty lines!" % skipped)
-        return dico
-
-    @staticmethod
-    def index_data(path, bin_path, dico):
-        """
-        Index sentences with a dictionary.
-        """
-        if bin_path is not None and os.path.isfile(bin_path):
-            print("Loading data from %s ..." % bin_path)
-            data = torch.load(bin_path)
-            assert dico == data["dico"]
-            return data
-
-        positions = []
-        sentences = []
-        unk_words = {}
-
-        # index sentences
-        f = open(path, "r", encoding="utf-8")
-        for i, line in enumerate(f):
-            if i % 1000000 == 0 and i > 0:
-                print(i)
-            s = line.rstrip().split()
-            # skip empty sentences
-            if len(s) == 0:
-                print("Empty sentence in line %i." % i)
-            # index sentence words
-            count_unk = 0
-            indexed = []
-            for w in s:
-                word_id = dico.index(w, no_unk=False)
-                # if we find a special word which is not an unknown word,
-                # skip the sentence
-                if 0 <= word_id < 4 + SPECIAL_WORDS and word_id != 3:
-                    logger.warning(
-                        'Found unexpected special word "%s" (%i)!!' % (w, word_id)
-                    )
-                    continue
-                assert word_id >= 0
-                indexed.append(word_id)
-                if word_id == dico.unk_index:
-                    unk_words[w] = unk_words.get(w, 0) + 1
-                    count_unk += 1
-            # add sentence
-            positions.append([len(sentences), len(sentences) + len(indexed)])
-            sentences.extend(indexed)
-            sentences.append(1)  # EOS index
-        f.close()
-
-        # tensorize data
-        positions = np.int64(positions)
-        if len(dico) < 1 << 16:
-            sentences = np.uint16(sentences)
-        elif len(dico) < 1 << 31:
-            sentences = np.int32(sentences)
-        else:
-            raise Exception("Dictionary is too big.")
-        assert sentences.min() >= 0
-        data = {
-            "dico": dico,
-            "positions": positions,
-            "sentences": sentences,
-            "unk_words": unk_words,
-        }
-        if bin_path is not None:
-            print("Saving the data to %s ..." % bin_path)
-            torch.save(data, bin_path, pickle_protocol=4)
-
-        return data
diff --git a/pytext/data/xlm_tensorizer.py b/pytext/data/xlm_tensorizer.py
deleted file mode 100644
index 764e9d2..0000000
--- a/pytext/data/xlm_tensorizer.py
+++ /dev/null
@@ -1,279 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
-
-import itertools
-from typing import Any, Dict, List, Optional, Tuple
-
-import torch
-from fairseq.data.legacy.masked_lm_dictionary import MaskedLMDictionary
-from pytext.config.component import ComponentType, create_component
-from pytext.data.bert_tensorizer import BERTTensorizer
-from pytext.data.tokenizers import Tokenizer
-from pytext.data.utils import BOS, EOS, MASK, PAD, UNK, Vocabulary, pad_and_tensorize
-from pytext.data.xlm_dictionary import Dictionary as XLMDictionary
-
-
-DEFAULT_LANG2ID_DICT = {
-    "ar": 0,
-    "bg": 1,
-    "de": 2,
-    "el": 3,
-    "en": 4,
-    "es": 5,
-    "fr": 6,
-    "hi": 7,
-    "ru": 8,
-    "sw": 9,
-    "th": 10,
-    "tr": 11,
-    "ur": 12,
-    "vi": 13,
-    "zh": 14,
-}
-
-
-class XLMTensorizer(BERTTensorizer):
-    """
-    Tensorizer for Cross-lingual LM tasks. Works for single sentence as well
-    as sentence pair.
-    """
-
-    class Config(BERTTensorizer.Config):
-        vocab_file: str = "/mnt/vol/nlp_technologies/xlm/vocab_xnli_15"
-        tokenizer: Tokenizer.Config = Tokenizer.Config()
-        is_fairseq: bool = False
-        pretraining: bool = False
-        max_seq_len: Optional[int] = 256
-        max_vocab: int = 95000
-        min_count: int = 0
-        language_columns: List[str] = ["language"]
-        lang2id: Dict[str, int] = DEFAULT_LANG2ID_DICT
-        reset_positions: bool = False
-        has_language_in_data: bool = False
-        use_language_embeddings: bool = True
-
-    @classmethod
-    def from_config(cls, config: Config):
-        tokenizer = create_component(ComponentType.TOKENIZER, config.tokenizer)
-        return cls(
-            columns=config.columns,
-            tokenizer=tokenizer,
-            vocab_file=config.vocab_file,
-            is_fairseq=config.is_fairseq,
-            pretraining=config.pretraining,
-            max_seq_len=config.max_seq_len,
-            max_vocab=config.max_vocab,
-            min_count=config.min_count,
-            language_columns=config.language_columns,
-            lang2id=config.lang2id,
-            reset_positions=config.reset_positions,
-            has_language_in_data=config.has_language_in_data,
-            use_language_embeddings=config.use_language_embeddings,
-        )
-
-    def __init__(
-        self,
-        columns=Config.columns,
-        tokenizer=None,
-        vocab_file=Config.vocab_file,
-        is_fairseq=Config.is_fairseq,
-        pretraining=Config.pretraining,
-        max_seq_len=Config.max_seq_len,
-        max_vocab=Config.max_vocab,
-        min_count=Config.min_count,
-        language_columns=Config.language_columns,
-        lang2id=Config.lang2id,
-        reset_positions=Config.reset_positions,
-        has_language_in_data=Config.has_language_in_data,
-        use_language_embeddings=Config.use_language_embeddings,
-    ) -> None:
-        assert (
-            len(columns) <= 2 and len(language_columns) <= 2
-        ), "Number of text fields and language columns cannot be greater than 2."
-
-        assert (
-            len(language_columns) == len(columns) or len(language_columns) == 1
-        ), "language columns must be same length as columns, or of length one"
-
-        # Used to distinguish the model pre-trained in PyText and the OSS FAIR model
-        self.is_fairseq = is_fairseq
-
-        # controls the settings we need explictly for pretraining
-        self.pretraining = pretraining
-
-        # language identifiers for extracting the language from a row of data
-        # during numberize
-        self.language_columns = language_columns
-
-        # language-to-id mapping used to obtain language embeddings
-        self.lang2id = lang2id
-
-        vocab = self._build_vocab(vocab_file, max_vocab, min_count)
-        self.special_token = vocab.idx[EOS]
-
-        # Controls whether we reset the position or not in case we have
-        # multiplt text fields
-        self.reset_positions = reset_positions
-
-        # controls whether we train with language embeddings or not
-        self.use_language_embeddings = use_language_embeddings
-
-        super().__init__(
-            columns=columns,
-            tokenizer=tokenizer,
-            add_bos_token=False,
-            add_eos_token=False,
-            use_eos_token_for_bos=True,
-            vocab=vocab,
-            max_seq_len=max_seq_len,
-        )
-        # if the dataset has a language column then adjust the schema appropriately
-        self.has_language_in_data = has_language_in_data
-
-    @property
-    def column_schema(self):
-        schema = super().column_schema
-        if self.has_language_in_data:
-            schema += [(column, str) for column in self.language_columns]
-        return schema
-
-    def _lookup_tokens(self, text: str, seq_len: int) -> List[int]:
-        tokenized_text = [t.value for t in self.tokenizer.tokenize(text)]
-        truncated_text = tokenized_text[:seq_len]
-        tokens = self.vocab.lookup_all(truncated_text)
-        return tokens
-
-    def _numberize_and_wrap(self, text: str, seq_len: int) -> List[List[int]]:
-        sentence = (
-            [self.special_token]
-            + self._lookup_tokens(text, seq_len)
-            + [self.special_token]
-        )
-        return [sentence]
-
-    def get_lang_id(self, row: Dict, col: str) -> int:
-        # generate lang embeddings. if training without lang embeddings, use
-        # the first language as the lang_id (there will always be one lang)
-        if self.use_language_embeddings:
-            lang = row[col]
-            assert lang in self.lang2id, f"language {lang} not supported in {row}"
-            lang_id = self.lang2id[lang]
-            return lang_id
-        else:
-            # use En as default
-            return self.lang2id.get("en", 0)
-
-    def numberize(self, row: Dict) -> Tuple[Any, ...]:
-        """
-        Extract text and language information from the current row of data being
-        processed. Process the text by tokenizing and vocabulary lookup. Convert
-        language to corresponding list of ids. This function can handle both
-        monolingual and parallel data as well as single sentence and sentence
-        pairs.
-        """
-        sentences = []
-        language_columns = self.language_columns
-        columns = self.columns
-
-        # When we have sentence pairs, we need to truncate each text field to
-        # max_seq_len // 2. However, the one case where this is not true
-        # is the case when we're pre-training MLM + TLM and the given row
-        # corresponds to monolingual data. Here the target text
-        # (text field related to columns[1]) is None and we don't need to adjust
-        # sequence length. Instead, we update columns and language_columns for
-        # this row. The following if statement handels this case.
-        if self.pretraining and len(columns) == 2:
-            if row[columns[1]] is None:
-                columns = columns[:1]
-                language_columns = ["language"]
-
-        # sequence_length is adjusted based on the number of text fields and needs
-        # to account for the special tokens which we will be wrapping
-        for column in columns:
-            sentences.extend(
-                self._numberize_and_wrap(
-                    text=row[column], seq_len=(self.max_seq_len // len(columns) - 2)
-                )
-            )
-
-        tokens = list(itertools.chain(*sentences))
-        seq_lens = [len(sentence) for sentence in sentences]
-
-        # create the language tensor. if only one language column is specified,
-        # use it for all texts
-        lang_ids = [self.get_lang_id(row, column) for column in language_columns]
-        if len(lang_ids) == 1:
-            lang_ids = lang_ids * len(self.columns)
-
-        # expand the language ids to each token
-        lang_ids = ([lang_id] * seq_len for lang_id, seq_len in zip(lang_ids, seq_lens))
-        lang_ids = list(itertools.chain(*lang_ids))
-
-        length = len(tokens)
-
-        if not self.reset_positions:
-            positions = [index for index in range(length)]
-        else:
-            positions = (range(seq_len) for seq_len in seq_lens)
-            positions = list(itertools.chain(*positions))
-        return tokens, length, lang_ids, positions
-
-    def sort_key(self, row):
-        return row[1]
-
-    def tensorize(self, batch) -> Tuple[torch.Tensor, ...]:
-        tokens, seq_lens, lang_ids, positions = zip(*batch)
-        padded_tokens = pad_and_tensorize(tokens, self.vocab.get_pad_index())
-        padded_lang_ids = pad_and_tensorize(lang_ids)
-        if self.is_fairseq:
-            positions = pad_and_tensorize(positions)
-        else:
-            positions = None
-        pad_mask = (padded_tokens != self.vocab.get_pad_index()).long()
-        return (
-            padded_tokens,
-            pad_mask,
-            pad_and_tensorize(seq_lens),
-            padded_lang_ids,
-            positions,
-        )
-
-    def _read_vocab(
-        self, vocab_file: str, max_vocab: int, min_count: int
-    ) -> Tuple[List, List, Dict]:
-        dictionary = XLMDictionary.read_vocab(vocab_file)
-        if max_vocab >= 1:
-            dictionary.max_vocab(max_vocab)
-        if min_count >= 0:
-            dictionary.min_count(min_count)
-        vocab_list = [dictionary.id2word[w] for w in sorted(dictionary.id2word)]
-        counts = [dictionary.counts[w] for w in vocab_list]
-        replacements = {"<unk>": UNK, "<pad>": PAD, "<s>": BOS, "</s>": EOS}
-        return vocab_list, counts, replacements
-
-    def _read_fairseq_vocab(
-        self, vocab_file: str, max_vocab: int = -1, min_count: int = -1
-    ) -> Tuple[List, List, Dict]:
-        dictionary = MaskedLMDictionary.load(vocab_file)
-        dictionary.finalize(threshold=min_count, nwords=max_vocab, padding_factor=1)
-        vocab_list = dictionary.symbols
-        counts = dictionary.count
-        replacements = {"<pad>": PAD, "</s>": EOS, "<unk>": UNK, "<mask>": MASK}
-        return vocab_list, counts, replacements
-
-    def _build_vocab(
-        self, vocab_file: str, max_vocab: int, min_count: int
-    ) -> Vocabulary:
-        """
-        Build Vocab for XLM by calling the vocab reader associated with the model
-        source.
-        """
-        if self.is_fairseq:
-            vocab_list, counts, replacements = self._read_fairseq_vocab(
-                vocab_file, max_vocab, min_count
-            )
-        else:
-            vocab_list, counts, replacements = self._read_vocab(
-                vocab_file, max_vocab, min_count
-            )
-        return Vocabulary(vocab_list, counts, replacements=replacements)
-- 
1.8.3.1


From 0ea49ef2f2e672d79a677ff0d9063bc06596f676 Mon Sep 17 00:00:00 2001
From: Deepali Chourasia <deepali@us.ibm.com>
Date: Tue, 27 Aug 2019 11:39:12 +0000
Subject: [PATCH 3/5] Revert "open source transformer representations (#736)"

This reverts commit 5ccbe1e8739b0c44a91d06a60f1b9c5ce2e34c32.

Conflicts:
	requirements.txt
---
 docs_requirements.txt                              |   2 -
 pytext/models/representations/attention.py         | 118 ------------------
 .../huggingface_bert_sentence_encoder.py           |  89 --------------
 .../transformer_sentence_encoder.py                | 136 ---------------------
 .../transformer_sentence_encoder_base.py           | 132 --------------------
 requirements.txt                                   |   2 -
 6 files changed, 479 deletions(-)
 delete mode 100644 pytext/models/representations/attention.py
 delete mode 100644 pytext/models/representations/huggingface_bert_sentence_encoder.py
 delete mode 100644 pytext/models/representations/transformer_sentence_encoder.py
 delete mode 100644 pytext/models/representations/transformer_sentence_encoder_base.py

diff --git a/docs_requirements.txt b/docs_requirements.txt
index 10a5fc6..bd82b50 100644
--- a/docs_requirements.txt
+++ b/docs_requirements.txt
@@ -1,13 +1,11 @@
 https://download.pytorch.org/whl/cpu/torch-1.1.0-cp37-cp37m-linux_x86_64.whl
 click
-fairseq @ git+https://github.com/pytorch/fairseq.git@master#egg=fairseq-0
 future
 hypothesis<4.0
 joblib
 numpy
 onnx
 pandas
-pytorch-pretrained-bert
 requests
 scipy
 torchtext
diff --git a/pytext/models/representations/attention.py b/pytext/models/representations/attention.py
deleted file mode 100644
index 10860a6..0000000
--- a/pytext/models/representations/attention.py
+++ /dev/null
@@ -1,118 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
-
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from pytext.models.module import Module
-
-
-class DotProductSelfAttention(Module):
-    """
-    Given vector w and token vectors = {t1, t2, ..., t_n}, compute self attention
-    weights to weighs the tokens
-    * a_j = softmax(w . t_j)
-    """
-
-    class Config(Module.Config):
-        input_dim: int = 32
-
-    @classmethod
-    def from_config(cls, config: Config):
-        return cls(config.input_dim)
-
-    def __init__(self, input_dim):
-        super().__init__()
-        self.linear = nn.Linear(input_dim, 1)
-
-    def forward(self, tokens, tokens_mask):
-        """
-        Input:
-            x: batch_size * seq_len * input_dim
-            x_mask: batch_size * seq_len (1 for padding, 0 for true)
-        Output:
-            alpha: batch_size * seq_len
-        """
-        scores = self.linear(tokens).squeeze(2)
-        scores.data.masked_fill_(tokens_mask.data, -float("inf"))
-        return F.softmax(scores, dim=-1)
-
-
-class SequenceAlignedAttention(Module):
-    """
-    Given sequences P and Q, computes attention weights for each element in P by
-    matching Q with each element in P.
-    * a_i_j = softmax(p_i . q_j) where softmax is computed by summing over q_j
-    """
-
-    class Config(Module.Config):
-        proj_dim: int = 32
-
-    @classmethod
-    def from_config(cls, config: Config):
-        return cls(config.proj_dim)
-
-    def __init__(self, proj_dim):
-        super().__init__()
-        self.linear = nn.Linear(proj_dim, proj_dim)
-        self.proj_dim = proj_dim
-
-    def forward(self, p: torch.Tensor, q: torch.Tensor, q_mask: torch.Tensor):
-        """
-        Input:
-            p: batch_size * p_seq_len * dim
-            q: batch_size * q_seq_len * dim
-            q_mask: batch_size * q_seq_len (1 for padding, 0 for true)
-        Output:
-            matched_seq: batch_size * doc_seq_len * dim
-        """
-        p_transform = F.relu(self.linear(p))
-        q_transform = F.relu(self.linear(q))
-
-        # Compute scores s_ij: bsz * doc_seq_len * ques_seq_len
-        attn_scores = p_transform.bmm(q_transform.transpose(2, 1))
-
-        # Mask padding: set a very low score for ques tokens that are pads.
-        q_mask = q_mask.unsqueeze(1).expand(attn_scores.size())
-        attn_scores.data.masked_fill_(q_mask.data, -float("inf"))
-
-        # Normalize with softmax: bsz * doc_seq_len * ques_seq_len
-        attn_scores_flattened = F.softmax(attn_scores.view(-1, q.size(1)), dim=-1)
-        return attn_scores_flattened.view(-1, p.size(1), q.size(1))
-
-
-class MultiplicativeAttention(Module):
-    """
-    Given sequence P and vector q, computes attention weights for each element
-    in P by matching q with each element in P using multiplicative attention.
-    * a_i = softmax(p_i . W . q)
-    """
-
-    class Config(Module.Config):
-        p_hidden_dim: int = 32
-        q_hidden_dim: int = 32
-        normalize: bool = False
-
-    @classmethod
-    def from_config(cls, config: Config):
-        return cls(config.p_hidden_dim, config.q_hidden_dim, config.normalize)
-
-    def __init__(self, p_hidden_dim, q_hidden_dim, normalize):
-        super().__init__()
-        self.normalize = normalize
-        self.linear = nn.Linear(p_hidden_dim, q_hidden_dim)
-
-    def forward(self, p_seq: torch.Tensor, q: torch.Tensor, p_mask: torch.Tensor):
-        """
-        Input:
-            p_seq: batch_size * p_seq_len * p_hidden_dim
-            q: batch_size * q_hidden_dim
-            p_mask: batch_size * p_seq_len (1 for padding, 0 for true)
-        Output:
-            attn_scores: batch_size * p_seq_len
-        """
-        Wq = self.linear(q) if self.linear is not None else q
-        pWq = p_seq.bmm(Wq.unsqueeze(2)).squeeze(2)
-        pWq.data.masked_fill_(p_mask.data, -float("inf"))
-        attn_scores = F.softmax(pWq, dim=-1) if self.normalize else pWq.exp()
-        return attn_scores
diff --git a/pytext/models/representations/huggingface_bert_sentence_encoder.py b/pytext/models/representations/huggingface_bert_sentence_encoder.py
deleted file mode 100644
index 434be61..0000000
--- a/pytext/models/representations/huggingface_bert_sentence_encoder.py
+++ /dev/null
@@ -1,89 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
-
-import os
-from typing import List, Tuple
-
-import torch
-from pytext.config import ConfigBase
-from pytext.models.representations.transformer_sentence_encoder_base import (
-    TransformerSentenceEncoderBase,
-)
-from pytorch_pretrained_bert.modeling import BertConfig, BertModel
-
-
-class HuggingFaceBertSentenceEncoder(TransformerSentenceEncoderBase):
-    """
-    Generate sentence representation using the open source HuggingFace BERT
-    model. This class implements loading the model weights from a
-    pre-trained model file.
-    """
-
-    class Config(TransformerSentenceEncoderBase.Config, ConfigBase):
-        bert_cpt_dir: str = "/mnt/vol/nlp_technologies/bert/uncased_L-12_H-768_A-12/"
-        load_weights: bool = True
-
-    def __init__(
-        self, config: Config, output_encoded_layers: bool, *args, **kwargs
-    ) -> None:
-        super().__init__(config, output_encoded_layers=output_encoded_layers)
-        # Load config
-        config_file = os.path.join(config.bert_cpt_dir, "bert_config.json")
-        bert_config = BertConfig.from_json_file(config_file)
-        print("Bert model config {}".format(bert_config))
-        # Instantiate model.
-        model = BertModel(bert_config)
-        weights_path = os.path.join(config.bert_cpt_dir, "pytorch_model.bin")
-        # load pre-trained weights if weights_path exists
-        if config.load_weights and os.path.isfile(weights_path):
-            state_dict = torch.load(weights_path)
-
-            missing_keys: List[str] = []
-            unexpected_keys: List[str] = []
-            error_msgs: List[str] = []
-            # copy state_dict so _load_from_state_dict can modify it
-            metadata = getattr(state_dict, "_metadata", None)
-            state_dict = state_dict.copy()
-            if metadata is not None:
-                state_dict._metadata = metadata
-
-            def load(module, prefix=""):
-                local_metadata = (
-                    {} if metadata is None else metadata.get(prefix[:-1], {})
-                )
-                module._load_from_state_dict(
-                    state_dict,
-                    prefix,
-                    local_metadata,
-                    True,
-                    missing_keys,
-                    unexpected_keys,
-                    error_msgs,
-                )
-                for name, child in module._modules.items():
-                    if child is not None:
-                        load(child, prefix + name + ".")
-
-            load(model, prefix="" if hasattr(model, "bert") else "bert.")
-            if len(missing_keys) > 0:
-                print(
-                    "Weights of {} not initialized from pretrained model: {}".format(
-                        model.__class__.__name__, missing_keys
-                    )
-                )
-            if len(unexpected_keys) > 0:
-                print(
-                    "Weights from pretrained model not used in {}: {}".format(
-                        model.__class__.__name__, unexpected_keys
-                    )
-                )
-
-        self.bert = model
-
-    def _encoder(self, input_tuple: Tuple[torch.Tensor, ...]):
-        tokens, pad_mask, segment_labels = input_tuple
-        return self.bert(tokens, segment_labels, pad_mask)
-
-    def _embedding(self):
-        # used to tie weights in MaskedLM model
-        return self.bert.embeddings.word_embeddings
diff --git a/pytext/models/representations/transformer_sentence_encoder.py b/pytext/models/representations/transformer_sentence_encoder.py
deleted file mode 100644
index f4d4099..0000000
--- a/pytext/models/representations/transformer_sentence_encoder.py
+++ /dev/null
@@ -1,136 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
-
-from typing import Tuple
-
-import torch
-from fairseq.modules import (
-    TransformerSentenceEncoder as TransformerSentenceEncoderModule,
-)
-from pytext.config import ConfigBase
-from pytext.models.representations.transformer_sentence_encoder_base import (
-    TransformerSentenceEncoderBase,
-)
-
-
-class TransformerSentenceEncoder(TransformerSentenceEncoderBase):
-    """
-    Implementation of the Transformer Sentence Encoder. This directly makes
-    use of the TransformerSentenceEncoder module in Fairseq.
-
-    A few interesting config options:
-        - encoder_normalize_before detemines whether the layer norm is applied
-          before or after self_attention. This is similar to original
-          implementation from Google.
-        - activation_fn can be set to 'gelu' instead of the default of 'relu'.
-        - project_representation adds a linear projection + tanh to the pooled output
-          in the style of BERT.
-    """
-
-    class Config(TransformerSentenceEncoderBase.Config, ConfigBase):
-        # Dropout parameters
-        dropout: float = 0.1
-        attention_dropout: float = 0.1
-        activation_dropout: float = 0.1
-
-        # Parameters related to hidden states and self-attention
-        embedding_dim: int = 768
-        ffn_embedding_dim: int = 3072
-        num_encoder_layers: int = 6
-        num_attention_heads: int = 8
-        num_segments: int = 2
-
-        # Parameters related to positions
-        use_position_embeddings: bool = True
-        # the fairseq module for position embeddings offsets all position
-        # ids by the padding index. Disable this offset by setting this flag
-        # to False. This will work correctly since we mask out the embeddings
-        # associated with padding in the encoder
-        offset_positions_by_padding: bool = True
-
-        # Model Initialization parameters
-        apply_bert_init: bool = True
-
-        # Misc. Params
-        encoder_normalize_before: bool = True
-        activation_fn: str = "relu"
-        project_representation: bool = False
-        max_seq_len: int = 128
-
-        # multilingual is set to true for cross-lingual LM training
-        multilingual: bool = False
-
-        # Flags for freezing parameters (e.g. during fine-tuning)
-        freeze_embeddings: bool = False
-        n_trans_layers_to_freeze: int = 0
-
-    def __init__(
-        self,
-        config: Config,
-        output_encoded_layers: bool,
-        padding_idx: int,
-        vocab_size: int,
-        *args,
-        **kwarg,
-    ) -> None:
-
-        super().__init__(config, output_encoded_layers=output_encoded_layers)
-        self.multilingual = config.multilingual
-        self.offset_positions_by_padding = config.offset_positions_by_padding
-
-        self.sentence_encoder = TransformerSentenceEncoderModule(
-            padding_idx=padding_idx,
-            vocab_size=vocab_size,
-            num_encoder_layers=config.num_encoder_layers,
-            embedding_dim=config.embedding_dim,
-            ffn_embedding_dim=config.ffn_embedding_dim,
-            num_attention_heads=config.num_attention_heads,
-            dropout=config.dropout,
-            attention_dropout=config.attention_dropout,
-            activation_dropout=config.activation_dropout,
-            max_seq_len=config.max_seq_len,
-            num_segments=config.num_segments,
-            use_position_embeddings=config.use_position_embeddings,
-            offset_positions_by_padding=config.offset_positions_by_padding,
-            encoder_normalize_before=config.encoder_normalize_before,
-            apply_bert_init=config.apply_bert_init,
-            activation_fn=config.activation_fn,
-            freeze_embeddings=config.freeze_embeddings,
-            n_trans_layers_to_freeze=config.n_trans_layers_to_freeze,
-            export=self.export,
-        )
-        self.projection = (
-            torch.nn.Linear(self.representation_dim, self.representation_dim)
-            if config.project_representation
-            else None
-        )
-
-    def _encoder(
-        self, input_tuple: Tuple[torch.Tensor, ...]
-    ) -> Tuple[torch.Tensor, ...]:
-        # If multilingual is True then the input_tuple has additional information
-        # related to the lengths of the inputs as well as a position tensor
-        if self.multilingual:
-            tokens, _, lengths, segment_labels, positions = input_tuple
-
-            # we need this for backwards compatibility with models that are
-            # pre-trained with the offset
-            if self.offset_positions_by_padding:
-                positions = None
-        else:
-            tokens, _, segment_labels = input_tuple
-            positions = None
-
-        encoded_layers, pooled_output = self.sentence_encoder(
-            tokens, segment_labels, positions=positions
-        )
-        # Each tensor in encoded_layers output by the Fairseq module has
-        # the shape: T x B x C. Convert this to B x T x C
-        encoded_layers = [x.transpose(0, 1) for x in encoded_layers]
-        if self.projection:
-            pooled_output = self.projection(pooled_output).tanh()
-        return encoded_layers, pooled_output
-
-    def _embedding(self):
-        # used to tie weights in MaskedLM model
-        return self.sentence_encoder.embed_tokens
diff --git a/pytext/models/representations/transformer_sentence_encoder_base.py b/pytext/models/representations/transformer_sentence_encoder_base.py
deleted file mode 100644
index 026569c..0000000
--- a/pytext/models/representations/transformer_sentence_encoder_base.py
+++ /dev/null
@@ -1,132 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
-
-from enum import Enum
-from typing import Tuple
-
-import torch
-import torch.nn as nn
-from pytext.config import ConfigBase
-from pytext.models.representations.representation_base import RepresentationBase
-
-
-class PoolingMethod(Enum):
-    """
-    Pooling Methods are chosen from the "Feature-based Approachs" section in
-    https://arxiv.org/pdf/1810.04805.pdf
-    """
-
-    AVG_CONCAT_LAST_4_LAYERS = "avg_concat_last_4_layers"
-    # since we only use hidden state of the first token from last layer for fine-tuning
-    AVG_SECOND_TO_LAST_LAYER = "avg_second_to_last_layer"
-    AVG_LAST_LAYER = "avg_last_layer"
-    AVG_SUM_LAST_4_LAYERS = "avg_sum_last_4_layers"
-    CLS_TOKEN = "cls_token"
-    NO_POOL = "no_pool"
-
-
-class TransformerSentenceEncoderBase(RepresentationBase):
-    """
-    Base class for all Bi-directional Transformer based Sentence Encoders. All
-    children of this class should implement an _encoder function which takes
-    as input: tokens, [optional] segment labels and a pad mask and outputs both
-    the sentence representation (output of _pool_encoded_layers) and the
-    output states of all the intermediate Transformer layers as a list of
-    tensors.
-
-    Input tuple consists of the following elements:
-    1) tokens: torch tensor of size B x T which contains tokens ids
-    2) pad_mask: torch tensor of size B x T generated with the condition
-    tokens != self.vocab.get_pad_index()
-    3) segment_labels: torch tensor of size B x T which contains the segment
-    id of each token
-
-    Output tuple consists of the following elements:
-    1) encoded_layers: List of torch tensors where each tensor has shape
-    B x T x C and there are num_transformer_layers + 1 of these.
-    Each tensor represents the output of the intermediate
-    transformer layers with the 0th element being the input to the
-    first transformer layer (token + segment + position emebdding).
-    2) [Optional] pooled_output: Output of the pooling operation associated
-    with config.pooling_method to the encoded_layers.
-    Size B x C (or B x 4C if pooling = AVG_CONCAT_LAST_4_LAYERS)
-    """
-
-    __EXPANSIBLE__ = True
-
-    class Config(RepresentationBase.Config, ConfigBase):
-        output_dropout: float = 0.4
-        embedding_dim: int = 768
-        pooling: PoolingMethod = PoolingMethod.CLS_TOKEN
-        export: bool = False
-
-    @classmethod
-    def from_config(cls, config: Config, output_encoded_layers=False, *args, **kwargs):
-        return cls(config, output_encoded_layers, *args, **kwargs)
-
-    def __init__(
-        self, config: Config, output_encoded_layers=False, *args, **kwargs
-    ) -> None:
-        super().__init__(config)
-        self.pooling = config.pooling
-        self.output_dropout = nn.Dropout(config.output_dropout)
-        self.output_encoded_layers = output_encoded_layers
-        self.export = config.export
-
-        assert (
-            self.pooling != PoolingMethod.NO_POOL or self.output_encoded_layers
-        ), "If PoolingMethod is no_pool then output_encoded_layers should be True"
-
-        if self.pooling == PoolingMethod.AVG_CONCAT_LAST_4_LAYERS:
-            self.representation_dim = config.embedding_dim * 4
-        else:
-            self.representation_dim = config.embedding_dim
-
-    def _encoder(
-        self, input_tuple: Tuple[torch.Tensor, ...]
-    ) -> Tuple[torch.Tensor, ...]:
-        raise NotImplementedError(
-            "Transformer Sentence Encoders should implement an encoder function"
-        )
-
-    def _pool_encoded_layers(
-        self, encoded_layers: torch.Tensor, pad_mask: torch.Tensor
-    ) -> torch.Tensor:
-        if self.pooling == PoolingMethod.NO_POOL:
-            return None
-        elif self.pooling == PoolingMethod.AVG_CONCAT_LAST_4_LAYERS:
-            sent_out = torch.cat(encoded_layers[-4:], 2)
-        elif self.pooling == PoolingMethod.AVG_SUM_LAST_4_LAYERS:
-            sent_out = torch.stack(encoded_layers[-4:]).sum(0)
-        elif self.pooling == PoolingMethod.AVG_SECOND_TO_LAST_LAYER:
-            sent_out = encoded_layers[-2]
-        elif self.pooling == PoolingMethod.AVG_LAST_LAYER:
-            sent_out = encoded_layers[-1]
-        else:
-            raise Exception("{} pooling is not supported".format(self.pooling))
-
-        pad_mask = pad_mask.unsqueeze(2)
-        sent_out = sent_out * pad_mask.float()
-        pooled_output = torch.sum(sent_out, 1) / torch.sum(pad_mask, 1).float()
-        return pooled_output
-
-    def forward(
-        self, input_tuple: Tuple[torch.Tensor, ...], *args
-    ) -> Tuple[torch.Tensor, ...]:
-
-        encoded_layers, pooled_output = self._encoder(input_tuple)
-
-        pad_mask = input_tuple[1]
-
-        if self.pooling != PoolingMethod.CLS_TOKEN:
-            pooled_output = self._pool_encoded_layers(encoded_layers, pad_mask)
-
-        if pooled_output is not None:
-            pooled_output = self.output_dropout(pooled_output)
-
-        output = []
-        if self.output_encoded_layers:
-            output.append(encoded_layers)
-        if self.pooling != PoolingMethod.NO_POOL:
-            output.append(pooled_output)
-        return tuple(output)
diff --git a/requirements.txt b/requirements.txt
index a4659d0..b3c447e 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,12 +1,10 @@
 click
-fairseq
 future
 hypothesis<4.0
 joblib
 numpy
 onnx
 pandas
-pytorch-pretrained-bert
 requests
 scipy
 torchtext
-- 
1.8.3.1


From dc10c75150729fe4f7ff2fa5ac6bc2151ce13195 Mon Sep 17 00:00:00 2001
From: Deepali Chourasia <deepali@us.ibm.com>
Date: Tue, 27 Aug 2019 11:41:01 +0000
Subject: [PATCH 4/5] Revert "Utilize faireq sparse transformer in pytext
 (#817)"

This reverts commit ea233ac885ee57f15d8c4549c478c7c0123770f2.
---
 .../sparse_transformer_sentence_encoder.py         | 108 ---------------------
 pytext/task/tasks.py                               |   3 -
 2 files changed, 111 deletions(-)
 delete mode 100644 pytext/models/representations/sparse_transformer_sentence_encoder.py

diff --git a/pytext/models/representations/sparse_transformer_sentence_encoder.py b/pytext/models/representations/sparse_transformer_sentence_encoder.py
deleted file mode 100644
index 366433e..0000000
--- a/pytext/models/representations/sparse_transformer_sentence_encoder.py
+++ /dev/null
@@ -1,108 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
-
-from fairseq.modules.sparse_transformer_sentence_encoder import (
-    SparseTransformerSentenceEncoder as SparseTransformerSentenceEncoderModule,
-)
-from pytext.config import ConfigBase
-from pytext.models.representations.transformer_sentence_encoder import (
-    TransformerSentenceEncoder,
-)
-
-
-class SparseTransformerSentenceEncoder(TransformerSentenceEncoder):
-    """
-    Implementation of the Transformer Sentence Encoder. This directly makes
-    use of the TransformerSentenceEncoder module in Fairseq.
-
-    A few interesting config options:
-        - encoder_normalize_before detemines whether the layer norm is applied
-          before or after self_attention. This is similar to original
-          implementation from Google.
-        - activation_fn can be set to 'gelu' instead of the default of 'relu'.
-        - project_representation adds a linear projection + tanh to the pooled output
-          in the style of BERT.
-    """
-
-    class Config(TransformerSentenceEncoder.Config, ConfigBase):
-        # Dropout parameters
-        dropout: float = 0.1
-        attention_dropout: float = 0.1
-        activation_dropout: float = 0.1
-
-        # Parameters related to hidden states and self-attention
-        embedding_dim: int = 768
-        ffn_embedding_dim: int = 3072
-        num_encoder_layers: int = 6
-        num_attention_heads: int = 8
-        num_segments: int = 2
-
-        # Parameters related to positions
-        use_position_embeddings: bool = True
-        # the fairseq module for position embeddings offsets all position
-        # ids by the padding index. Disable this offset by setting this flag
-        # to False. This will work correctly since we mask out the embeddings
-        # associated with padding in the encoder
-        offset_positions_by_padding: bool = True
-
-        # Model Initialization parameters
-        apply_bert_init: bool = True
-
-        # Misc. Params
-        encoder_normalize_before: bool = True
-        activation_fn: str = "relu"
-        project_representation: bool = False
-        max_seq_len: int = 128
-
-        # multilingual is set to true for cross-lingual LM training
-        multilingual: bool = False
-
-        # Flags for freezing parameters (e.g. during fine-tuning)
-        freeze_embeddings: bool = False
-        n_trans_layers_to_freeze: int = 0
-
-        # Sparse multihead attention parameters
-        is_bidirectional: bool = True
-        stride: int = 32
-        expressivity: int = 8
-
-    def __init__(
-        self,
-        config: Config,
-        output_encoded_layers: bool,
-        padding_idx: int,
-        vocab_size: int,
-        *args,
-        **kwarg,
-    ) -> None:
-
-        super().__init__(
-            config,
-            output_encoded_layers=output_encoded_layers,
-            padding_idx=padding_idx,
-            vocab_size=vocab_size,
-        )
-        self.sentence_encoder = SparseTransformerSentenceEncoderModule(
-            padding_idx=padding_idx,
-            vocab_size=vocab_size,
-            num_encoder_layers=config.num_encoder_layers,
-            embedding_dim=config.embedding_dim,
-            ffn_embedding_dim=config.ffn_embedding_dim,
-            num_attention_heads=config.num_attention_heads,
-            dropout=config.dropout,
-            attention_dropout=config.attention_dropout,
-            activation_dropout=config.activation_dropout,
-            max_seq_len=config.max_seq_len,
-            num_segments=config.num_segments,
-            use_position_embeddings=config.use_position_embeddings,
-            offset_positions_by_padding=config.offset_positions_by_padding,
-            encoder_normalize_before=config.encoder_normalize_before,
-            apply_bert_init=config.apply_bert_init,
-            activation_fn=config.activation_fn,
-            freeze_embeddings=config.freeze_embeddings,
-            n_trans_layers_to_freeze=config.n_trans_layers_to_freeze,
-            export=self.export,
-            is_bidirectional=config.is_bidirectional,
-            stride=config.stride,
-            expressivity=config.expressivity,
-        )
diff --git a/pytext/task/tasks.py b/pytext/task/tasks.py
index 47fa139..4b8df8c 100644
--- a/pytext/task/tasks.py
+++ b/pytext/task/tasks.py
@@ -44,9 +44,6 @@ from pytext.models.query_document_pairwise_ranking_model import (
     QueryDocPairwiseRankingModel,
     QueryDocumentPairwiseRankingModel_Deprecated,
 )
-from pytext.models.representations.sparse_transformer_sentence_encoder import (  # noqa f401
-    SparseTransformerSentenceEncoder,
-)
 from pytext.models.semantic_parsers.rnng.rnng_parser import (
     RNNGParser,
     RNNGParser_Deprecated,
-- 
1.8.3.1


From 1a7d7932a2010a4b78e164874047e2bf50cc8464 Mon Sep 17 00:00:00 2001
From: Deepali Chourasia <deepali@us.ibm.com>
Date: Tue, 27 Aug 2019 11:58:40 +0000
Subject: [PATCH 5/5] Revert "open source extractive question answering models
 (#742)"

This reverts commit f62dfa539b3319828631cec0943f888442ba61ca.

Conflicts:
	pytext/data/test/tensorizers_test.py
	pytext/models/qna/dr_qa.py
	pytext/task/tasks.py
---
 pytext/builtin_task.py                             |   2 -
 pytext/data/squad_for_bert_tensorizer.py           |  86 ------
 pytext/data/squad_tensorizer.py                    | 217 -------------
 pytext/data/test/tensorizers_test.py               | 152 +---------
 pytext/metric_reporters/__init__.py                |   2 -
 pytext/metric_reporters/squad_metric_reporter.py   | 335 ---------------------
 pytext/metrics/squad_metrics.py                    |  15 -
 pytext/models/output_layers/squad_output_layer.py  | 171 -----------
 pytext/models/qna/bert_squad_qa.py                 | 107 -------
 pytext/models/qna/dr_qa.py                         | 216 -------------
 .../test/transformer_sentence_encoder_test.py      |  87 ------
 pytext/task/tasks.py                               |   9 -
 12 files changed, 2 insertions(+), 1397 deletions(-)
 delete mode 100644 pytext/data/squad_for_bert_tensorizer.py
 delete mode 100644 pytext/data/squad_tensorizer.py
 delete mode 100644 pytext/metric_reporters/squad_metric_reporter.py
 delete mode 100644 pytext/metrics/squad_metrics.py
 delete mode 100644 pytext/models/output_layers/squad_output_layer.py
 delete mode 100644 pytext/models/qna/bert_squad_qa.py
 delete mode 100644 pytext/models/qna/dr_qa.py
 delete mode 100644 pytext/models/test/transformer_sentence_encoder_test.py

diff --git a/pytext/builtin_task.py b/pytext/builtin_task.py
index d5d2a89..f2ada8a 100644
--- a/pytext/builtin_task.py
+++ b/pytext/builtin_task.py
@@ -26,7 +26,6 @@ from pytext.task.tasks import (
     SemanticParsingTask_Deprecated,
     SeqNNTask,
     SeqNNTask_Deprecated,
-    SquadQATask,
     WordTaggingTask,
     WordTaggingTask_Deprecated,
 )
@@ -81,7 +80,6 @@ def register_builtin_tasks():
             SemanticParsingTask_Deprecated,
             SeqNNTask,
             SeqNNTask_Deprecated,
-            SquadQATask,
             WordTaggingTask,
             WordTaggingTask_Deprecated,
         )
diff --git a/pytext/data/squad_for_bert_tensorizer.py b/pytext/data/squad_for_bert_tensorizer.py
deleted file mode 100644
index 215a990..0000000
--- a/pytext/data/squad_for_bert_tensorizer.py
+++ /dev/null
@@ -1,86 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
-
-import itertools
-from typing import List
-
-from pytext.data.bert_tensorizer import BERTTensorizer
-from pytext.data.utils import BOS, pad_and_tensorize
-
-
-class SquadForBERTTensorizer(BERTTensorizer):
-    """Produces BERT inputs and answer spans for Squad."""
-
-    SPAN_PAD_IDX = -100
-
-    class Config(BERTTensorizer.Config):
-        columns: List[str] = ["question", "doc"]
-        # for labels
-        answers_column: str = "answers"
-        answer_starts_column: str = "answer_starts"
-        max_seq_len: int = 256
-
-    @classmethod
-    def from_config(cls, config: Config):
-        # reuse parent class's from_config, which will pass extra args
-        # in **kwargs to cls.__init__
-        return super().from_config(
-            config,
-            answers_column=config.answers_column,
-            answer_starts_column=config.answer_starts_column,
-        )
-
-    def __init__(
-        self,
-        answers_column: str = Config.answers_column,
-        answer_starts_column: str = Config.answer_starts_column,
-        **kwargs,
-    ):
-        super().__init__(**kwargs)
-        self.answers_column = answers_column
-        self.answer_starts_column = answer_starts_column
-
-    def numberize(self, row):
-        question_column, doc_column = self.columns
-        doc_tokens, start_idx, end_idx = self._lookup_tokens(row[doc_column])
-        question_tokens, _, _ = self._lookup_tokens(row[question_column])
-        question_tokens = [self.vocab.idx[BOS]] + question_tokens
-        seq_lens = (len(question_tokens), len(doc_tokens))
-        segment_labels = ([i] * seq_len for i, seq_len in enumerate(seq_lens))
-        tokens = list(itertools.chain(question_tokens, doc_tokens))
-        segment_labels = list(itertools.chain(*segment_labels))
-        seq_len = len(tokens)
-
-        # now map original answer spans to tokenized spans
-        offset = len(question_tokens)
-        start_idx_map = {}
-        end_idx_map = {}
-        for tokenized_idx, (raw_start_idx, raw_end_idx) in enumerate(
-            zip(start_idx[:-1], end_idx[:-1])
-        ):
-            start_idx_map[raw_start_idx] = tokenized_idx + offset
-            end_idx_map[raw_end_idx] = tokenized_idx + offset
-
-        answer_start_indices = [
-            start_idx_map.get(raw_idx, self.SPAN_PAD_IDX)
-            for raw_idx in row[self.answer_starts_column]
-        ]
-        answer_end_indices = [
-            end_idx_map.get(raw_idx + len(answer), self.SPAN_PAD_IDX)
-            for raw_idx, answer in zip(
-                row[self.answer_starts_column], row[self.answers_column]
-            )
-        ]
-        if not answer_start_indices and answer_end_indices:
-            answer_start_indices = [self.SPAN_PAD_IDX]
-            answer_end_indices = [self.SPAN_PAD_IDX]
-        return tokens, segment_labels, seq_len, answer_start_indices, answer_end_indices
-
-    def tensorize(self, batch):
-        tokens, segment_labels, seq_lens, answer_start_idx, answer_end_idx = zip(*batch)
-        tokens = pad_and_tensorize(tokens, self.vocab.get_pad_index())
-        segment_labels = pad_and_tensorize(segment_labels, self.vocab.get_pad_index())
-        pad_mask = (tokens != self.vocab.get_pad_index()).long()
-        answer_start_idx = pad_and_tensorize(answer_start_idx, self.SPAN_PAD_IDX)
-        answer_end_idx = pad_and_tensorize(answer_end_idx, self.SPAN_PAD_IDX)
-        return tokens, pad_mask, segment_labels, answer_start_idx, answer_end_idx
diff --git a/pytext/data/squad_tensorizer.py b/pytext/data/squad_tensorizer.py
deleted file mode 100644
index 6c6aa27..0000000
--- a/pytext/data/squad_tensorizer.py
+++ /dev/null
@@ -1,217 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
-
-from typing import List
-
-from pytext.config.component import ComponentType, create_component
-from pytext.data.tensorizers import TokenTensorizer
-from pytext.data.tokenizers import Tokenizer, WordPieceTokenizer
-from pytext.data.utils import (
-    BOS,
-    EOS,
-    MASK,
-    PAD,
-    UNK,
-    VocabBuilder,
-    Vocabulary,
-    pad_and_tensorize,
-)
-
-
-class SquadTensorizer(TokenTensorizer):
-    """Produces inputs and answer spans for Squad."""
-
-    SPAN_PAD_IDX = -100
-
-    class Config(TokenTensorizer.Config):
-        # for model inputs
-        doc_column: str = "doc"
-        ques_column: str = "question"
-        # for labels
-        answers_column: str = "answers"
-        answer_starts_column: str = "answer_starts"
-        # Since Tokenizer is __EXPANSIBLE__, we don't need a Union type to
-        # support WordPieceTokenizer.
-        tokenizer: Tokenizer.Config = Tokenizer.Config(split_regex=r"\W+")
-        max_ques_seq_len: int = 64
-        max_doc_seq_len: int = 256
-
-    @classmethod
-    def from_config(cls, config: Config, **kwargs):
-        tokenizer = create_component(ComponentType.TOKENIZER, config.tokenizer)
-        vocab = None
-        if isinstance(tokenizer, WordPieceTokenizer):
-            print("Using WordPieceTokenizer")
-            replacements = {
-                "[UNK]": UNK,
-                "[PAD]": PAD,
-                "[CLS]": BOS,
-                "[SEP]": EOS,
-                "[MASK]": MASK,
-            }
-            vocab = Vocabulary(
-                [token for token, _ in tokenizer.vocab.items()],
-                replacements=replacements,
-            )
-
-        doc_tensorizer = TokenTensorizer(
-            text_column=config.doc_column,
-            tokenizer=tokenizer,
-            vocab=vocab,
-            max_seq_len=config.max_doc_seq_len,
-        )
-        ques_tensorizer = TokenTensorizer(
-            text_column=config.ques_column,
-            tokenizer=tokenizer,
-            vocab=vocab,
-            max_seq_len=config.max_ques_seq_len,
-        )
-        return cls(
-            doc_tensorizer,
-            ques_tensorizer,
-            doc_column=config.doc_column,
-            ques_column=config.ques_column,
-            answers_column=config.answers_column,
-            answer_starts_column=config.answer_starts_column,
-            tokenizer=tokenizer,
-            vocab=vocab,
-            **kwargs,
-        )
-
-    def __init__(
-        self,
-        doc_tensorizer: TokenTensorizer,
-        ques_tensorizer: TokenTensorizer,
-        doc_column: str = Config.doc_column,
-        ques_column: str = Config.ques_column,
-        answers_column: str = Config.answers_column,
-        answer_starts_column: str = Config.answer_starts_column,
-        **kwargs,
-    ):
-        super().__init__(text_column=None, **kwargs)
-        self.ques_tensorizer = ques_tensorizer
-        self.doc_tensorizer = doc_tensorizer
-        self.doc_column = doc_column
-        self.ques_column = ques_column
-        self.answers_column = answers_column
-        self.answer_starts_column = answer_starts_column
-
-    def initialize(self, vocab_builder=None):
-        """Build vocabulary based on training corpus."""
-        if not self.vocab:
-            self.vocab_builder = vocab_builder or VocabBuilder()
-            self.vocab_builder.pad_index = 0
-            self.vocab_builder.unk_index = 1
-            ques_initializer = self.ques_tensorizer.initialize(self.vocab_builder)
-            doc_initializer = self.doc_tensorizer.initialize(self.vocab_builder)
-            ques_initializer.send(None)
-            doc_initializer.send(None)
-        try:
-            while True:
-                if self.vocab:
-                    yield
-                else:
-                    row = yield
-                    ques_initializer.send(row)
-                    doc_initializer.send(row)
-        except GeneratorExit:
-            if not self.vocab:
-                self.vocab = self.vocab_builder.make_vocab()
-
-    def _lookup_tokens(self, text, source_is_doc=True):
-        # This is useful in SquadMetricReporter._unnumberize()
-        return (
-            self.doc_tensorizer._lookup_tokens(text)
-            if source_is_doc
-            else self.ques_tensorizer._lookup_tokens(text)
-        )
-
-    def numberize(self, row):
-        assert len(self.vocab) == len(self.ques_tensorizer.vocab)
-        assert len(self.vocab) == len(self.doc_tensorizer.vocab)
-
-        # Do NOT use self._lookup_tokens() because it won't enforce max_ques_seq_len.
-        ques_tokens, _, _ = self.ques_tensorizer._lookup_tokens(row[self.ques_column])
-
-        # Start and end indices are those of the tokens in original text.
-        # The behavior doesn't change for WordPieceTokenizer because...
-        # If there's a word piece, say, "##ly" then the start and end indices
-        # will be that of "ly" in original text. These are also char level.
-        doc_tokens, orig_start_idx, orig_end_idx = self.doc_tensorizer._lookup_tokens(
-            row[self.doc_column]
-        )
-
-        # Now map original character level answer spans to token level spans
-        start_idx_map = {}
-        end_idx_map = {}
-        for token_idx, (start_idx, end_idx) in enumerate(
-            zip(orig_start_idx, orig_end_idx)
-        ):
-            start_idx_map[start_idx] = token_idx
-            end_idx_map[end_idx] = token_idx
-
-        answer_start_token_indices = [
-            start_idx_map.get(raw_idx, self.SPAN_PAD_IDX)
-            for raw_idx in row[self.answer_starts_column]
-        ]
-        answer_end_token_indices = [
-            end_idx_map.get(raw_idx + len(answer), self.SPAN_PAD_IDX)
-            for raw_idx, answer in zip(
-                row[self.answer_starts_column], row[self.answers_column]
-            )
-        ]  # The end index is inclusive. Span = doc_tokens[start:end+1]
-
-        if (
-            not (answer_start_token_indices and answer_end_token_indices)
-            or self._only_pad(answer_start_token_indices)
-            or self._only_pad(answer_end_token_indices)
-        ):
-            answer_start_token_indices = [self.SPAN_PAD_IDX]
-            answer_end_token_indices = [self.SPAN_PAD_IDX]
-
-        return (
-            doc_tokens,
-            len(doc_tokens),
-            ques_tokens,
-            len(ques_tokens),
-            answer_start_token_indices,
-            answer_end_token_indices,
-        )
-
-    def tensorize(self, batch):
-        (
-            doc_tokens,
-            doc_seq_len,
-            ques_tokens,
-            ques_seq_len,
-            answer_start_idx,
-            answer_end_idx,
-        ) = zip(*batch)
-        doc_tokens = pad_and_tensorize(doc_tokens, self.vocab.get_pad_index())
-        doc_mask = (doc_tokens == self.vocab.get_pad_index()).byte()  # 1 => pad
-        ques_tokens = pad_and_tensorize(ques_tokens, self.vocab.get_pad_index())
-        ques_mask = (ques_tokens == self.vocab.get_pad_index()).byte()  # 1 => pad
-        answer_start_idx = pad_and_tensorize(answer_start_idx, self.SPAN_PAD_IDX)
-        answer_end_idx = pad_and_tensorize(answer_end_idx, self.SPAN_PAD_IDX)
-
-        # doc_tokens must be returned as the first element for
-        # SquadMetricReporter._add_decoded_answer_batch_stats() to work
-        return (
-            doc_tokens,
-            pad_and_tensorize(doc_seq_len),
-            doc_mask,
-            ques_tokens,
-            pad_and_tensorize(ques_seq_len),
-            ques_mask,
-            answer_start_idx,
-            answer_end_idx,
-        )
-
-    def sort_key(self, row):
-        raise NotImplementedError("SquadTensorizer.sort_key() should not be called.")
-
-    def _only_pad(self, token_id_list: List[int]) -> bool:
-        for token_id in token_id_list:
-            if token_id != self.SPAN_PAD_IDX:
-                return False
-        return True
diff --git a/pytext/data/test/tensorizers_test.py b/pytext/data/test/tensorizers_test.py
index 482f2fb..2769eec 100644
--- a/pytext/data/test/tensorizers_test.py
+++ b/pytext/data/test/tensorizers_test.py
@@ -6,11 +6,8 @@ from typing import List
 
 import numpy as np
 import torch
-from pytext.data.sources import SquadDataSource
 from pytext.data.sources.data_source import Gazetteer, SafeFileWrapper, load_float_list
 from pytext.data.sources.tsv import SessionTSVDataSource, TSVDataSource
-from pytext.data.squad_for_bert_tensorizer import SquadForBERTTensorizer
-from pytext.data.squad_tensorizer import SquadTensorizer
 from pytext.data.tensorizers import (
     AnnotationNumberizer,
     ByteTensorizer,
@@ -25,6 +22,8 @@ from pytext.data.tensorizers import (
     VocabFileConfig,
     initialize_tensorizers,
 )
+
+from pytext.data.tokenizers import Tokenizer
 from pytext.utils.test import import_tests_module
 
 
@@ -563,150 +562,3 @@ class TensorizersTest(unittest.TestCase):
         for row, expected in zip(data.train, EXPECTED_ACTIONS):
             actions = nbrz.numberize(row)
             self.assertEqual(expected, actions)
-
-class SquadForBERTTensorizerTest(unittest.TestCase):
-    def test_squad_tensorizer(self):
-        source = SquadDataSource.from_config(
-            SquadDataSource.Config(
-                eval_filename=tests_module.test_file("squad_tiny.json")
-            )
-        )
-        row = next(iter(source.eval))
-        tensorizer = SquadForBERTTensorizer.from_config(
-            SquadForBERTTensorizer.Config(
-                tokenizer=WordPieceTokenizer.Config(
-                    wordpiece_vocab_path="pytext/data/test/data/wordpiece_1k.txt"
-                ),
-                max_seq_len=250,
-            )
-        )
-        tokens, segments, seq_len, start, end = tensorizer.numberize(row)
-        # check against manually verified answer positions in tokenized output
-        # there are 4 identical answers
-        self.assertEqual(start, [83, 83, 83, 83])
-        self.assertEqual(end, [87, 87, 87, 87])
-        self.assertEqual(len(tokens), seq_len)
-        self.assertEqual(len(segments), seq_len)
-
-        tensorizer.max_seq_len = 50
-        # answer should be truncated out
-        _, _, _, start, end = tensorizer.numberize(row)
-        self.assertEqual(start, [-100, -100, -100, -100])
-        self.assertEqual(end, [-100, -100, -100, -100])
-        self.assertEqual(len(tokens), seq_len)
-        self.assertEqual(len(segments), seq_len)
-
-
-class SquadTensorizerTest(unittest.TestCase):
-    def setUp(self):
-        self.data_source = SquadDataSource.from_config(
-            SquadDataSource.Config(
-                train_filename=tests_module.test_file("squad_tiny.json"),
-                eval_filename=None,
-                test_filename=None,
-            )
-        )
-        self.tensorizer_with_wordpiece = SquadTensorizer.from_config(
-            SquadTensorizer.Config(
-                tokenizer=WordPieceTokenizer.Config(
-                    wordpiece_vocab_path="pytext/data/test/data/wordpiece_1k.txt"
-                ),
-                max_seq_len=250,
-            )
-        )
-        self.tensorizer_with_alphanumeric = SquadTensorizer.from_config(
-            SquadTensorizer.Config(
-                tokenizer=Tokenizer.Config(split_regex=r"\W+"), max_seq_len=250
-            )
-        )
-
-    def _init_tensorizer(self):
-        tensorizer_dict = {
-            "wordpiece": self.tensorizer_with_wordpiece,
-            "alphanumeric": self.tensorizer_with_alphanumeric,
-        }
-        initialize_tensorizers(tensorizer_dict, self.data_source.train)
-
-    def test_initialize(self):
-        self._init_tensorizer()
-        self.assertEqual(len(self.tensorizer_with_wordpiece.vocab), 1000)
-        self.assertEqual(
-            len(self.tensorizer_with_wordpiece.ques_tensorizer.vocab), 1000
-        )
-        self.assertEqual(len(self.tensorizer_with_wordpiece.doc_tensorizer.vocab), 1000)
-        self.assertEqual(len(self.tensorizer_with_alphanumeric.vocab), 1418)
-        self.assertEqual(
-            len(self.tensorizer_with_alphanumeric.ques_tensorizer.vocab), 1418
-        )
-        self.assertEqual(
-            len(self.tensorizer_with_alphanumeric.doc_tensorizer.vocab), 1418
-        )
-
-    def test_numberize_with_alphanumeric(self):
-        self._init_tensorizer()
-        row = next(iter(self.data_source.train))
-        (
-            doc_tokens,
-            doc_seq_len,
-            ques_tokens,
-            ques_seq_len,
-            answer_start_token_idx,
-            answer_end_token_idx,
-        ) = self.tensorizer_with_alphanumeric.numberize(row)
-
-        # check against manually verified answer positions in tokenized output
-        # there are 4 identical answers
-        self.assertEqual(len(ques_tokens), ques_seq_len)
-        self.assertEqual(len(doc_tokens), doc_seq_len)
-        self.assertEqual(ques_tokens, [2, 3, 4, 5, 6, 7])  # It's a coincidence.
-        self.assertEqual(answer_start_token_idx, [26, 26, 26, 26])
-        self.assertEqual(answer_end_token_idx, [26, 26, 26, 26])
-
-        self.tensorizer_with_alphanumeric.doc_tensorizer.max_seq_len = 20
-        # answer should be truncated out because max doc len is smaller.
-        (
-            doc_tokens,
-            doc_seq_len,
-            ques_tokens,
-            ques_seq_len,
-            answer_start_token_idx,
-            answer_end_token_idx,
-        ) = self.tensorizer_with_alphanumeric.numberize(row)
-        self.assertEqual(len(ques_tokens), ques_seq_len)
-        self.assertEqual(len(doc_tokens), doc_seq_len)
-        self.assertEqual(answer_start_token_idx, [-100])
-        self.assertEqual(answer_end_token_idx, [-100])
-
-    def test_numberize_with_wordpiece(self):
-        self._init_tensorizer()
-        row = next(iter(self.data_source.train))
-        (
-            doc_tokens,
-            doc_seq_len,
-            ques_tokens,
-            ques_seq_len,
-            answer_start_token_idx,
-            answer_end_token_idx,
-        ) = self.tensorizer_with_wordpiece.numberize(row)
-
-        # check against manually verified answer positions in tokenized output
-        # there are 4 identical answers
-        self.assertEqual(len(ques_tokens), ques_seq_len)
-        self.assertEqual(len(doc_tokens), doc_seq_len)
-        self.assertEqual(answer_start_token_idx, [70, 70, 70, 70])
-        self.assertEqual(answer_end_token_idx, [74, 74, 74, 74])
-
-        self.tensorizer_with_wordpiece.doc_tensorizer.max_seq_len = 50
-        # answer should be truncated out because max doc len is smaller.
-        (
-            doc_tokens,
-            doc_seq_len,
-            ques_tokens,
-            ques_seq_len,
-            answer_start_token_idx,
-            answer_end_token_idx,
-        ) = self.tensorizer_with_wordpiece.numberize(row)
-        self.assertEqual(len(ques_tokens), ques_seq_len)
-        self.assertEqual(len(doc_tokens), doc_seq_len)
-        self.assertEqual(answer_start_token_idx, [-100])
-        self.assertEqual(answer_end_token_idx, [-100])
diff --git a/pytext/metric_reporters/__init__.py b/pytext/metric_reporters/__init__.py
index 857f8dd..223b675 100644
--- a/pytext/metric_reporters/__init__.py
+++ b/pytext/metric_reporters/__init__.py
@@ -12,7 +12,6 @@ from .language_model_metric_reporter import LanguageModelMetricReporter
 from .metric_reporter import MetricReporter
 from .pairwise_ranking_metric_reporter import PairwiseRankingMetricReporter
 from .regression_metric_reporter import RegressionMetricReporter
-from .squad_metric_reporter import SquadMetricReporter
 from .word_tagging_metric_reporter import (
     SequenceTaggingMetricReporter,
     WordTaggingMetricReporter,
@@ -27,7 +26,6 @@ __all__ = [
     "RegressionMetricReporter",
     "IntentSlotMetricReporter",
     "LanguageModelMetricReporter",
-    "SquadMetricReporter",
     "WordTaggingMetricReporter",
     "CompositionalMetricReporter",
     "PairwiseRankingMetricReporter",
diff --git a/pytext/metric_reporters/squad_metric_reporter.py b/pytext/metric_reporters/squad_metric_reporter.py
deleted file mode 100644
index 7dbc81b..0000000
--- a/pytext/metric_reporters/squad_metric_reporter.py
+++ /dev/null
@@ -1,335 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
-
-import re
-import string
-from collections import Counter
-from typing import Dict, List
-
-from pytext.common.constants import RawExampleFieldName, Stage
-from pytext.metric_reporters.channel import Channel, ConsoleChannel, FileChannel
-from pytext.metric_reporters.metric_reporter import MetricReporter
-from pytext.metrics.squad_metrics import SquadMetrics
-
-
-class SquadFileChannel(FileChannel):
-    def get_title(self):
-        return (
-            "index",
-            "ques",
-            "doc",
-            "predicted_answer",
-            "true_answers",
-            "predicted_start_pos",
-            "predicted_end_pos",
-            "true_start_pos",
-            "true_end_pos",
-            "start_pos_scores",
-            "end_pos_scores",
-            "predicted_has_answer",
-            "true_has_answer",
-            "has_answer_scores",
-        )
-
-    def gen_content(self, metrics, loss, preds, targets, scores, contexts, *args):
-        pred_answers, pred_start_pos, pred_end_pos, pred_has_answer = preds
-        true_answers, true_start_pos, true_end_pos, true_has_answer = targets
-        start_pos_scores, end_pos_scores, has_answer_scores = scores
-        for i in range(len(pred_answers)):
-            yield [
-                contexts[RawExampleFieldName.ROW_INDEX][i],
-                contexts[SquadMetricReporter.QUES_COLUMN][i],
-                contexts[SquadMetricReporter.DOC_COLUMN][i],
-                pred_answers[i],
-                true_answers[i],
-                pred_start_pos[i],
-                pred_end_pos[i],
-                true_start_pos[i],
-                true_end_pos[i],
-                start_pos_scores[i],
-                end_pos_scores[i],
-                pred_has_answer[i],
-                true_has_answer[i],
-                has_answer_scores[i],
-            ]
-
-
-class SquadMetricReporter(MetricReporter):
-    QUES_COLUMN = "question"
-    ANSWERS_COLUMN = "answers"
-    DOC_COLUMN = "doc"
-    ROW_INDEX = "row_index"
-
-    class Config(MetricReporter.Config):
-        n_best_size: int = 5
-        max_answer_length: int = 16
-        ignore_impossible: bool = True
-        false_label: str = "False"
-
-    @classmethod
-    def from_config(cls, config, *args, tensorizers=None, **kwargs):
-        return cls(
-            channels=[
-                ConsoleChannel(),
-                SquadFileChannel((Stage.TEST,), config.output_path),
-            ],
-            n_best_size=config.n_best_size,
-            max_answer_length=config.max_answer_length,
-            ignore_impossible=config.ignore_impossible,
-            has_answer_labels=tensorizers["has_answer"].vocab._vocab,
-            tensorizer=tensorizers["squad_input"],
-            false_label=config.false_label,
-        )
-
-    def __init__(
-        self,
-        channels: List[Channel],
-        n_best_size: int,
-        max_answer_length: int,
-        ignore_impossible: bool,
-        has_answer_labels: List[str],
-        tensorizer=None,
-        false_label=Config.false_label,
-    ) -> None:
-        super().__init__(channels)
-        self.channels = channels
-        self.tensorizer = tensorizer
-        self.ignore_impossible = ignore_impossible
-        self.has_answer_labels = has_answer_labels
-        self.false_label = false_label
-        self.false_idx = 1 if has_answer_labels[1] == false_label else 0
-        self.true_idx = 1 - self.false_idx
-
-    def _reset(self):
-        self.all_start_pos_preds: List = []
-        self.all_start_pos_targets: List = []
-        self.all_start_pos_scores: List = []
-        self.all_end_pos_preds: List = []
-        self.all_end_pos_targets: List = []
-        self.all_end_pos_scores: List = []
-        self.all_has_answer_targets: List = []
-        self.all_has_answer_preds: List = []
-        self.all_has_answer_scores: List = []
-
-        self.all_preds = (
-            self.all_start_pos_preds,
-            self.all_end_pos_preds,
-            self.all_has_answer_preds,
-        )
-        self.all_targets = (
-            self.all_start_pos_targets,
-            self.all_end_pos_targets,
-            self.all_has_answer_targets,
-        )
-        self.all_scores = (
-            self.all_start_pos_scores,
-            self.all_end_pos_scores,
-            self.all_has_answer_scores,
-        )
-        self.all_context: Dict = {}
-        self.all_loss: List = []
-        self.all_pred_answers: List = []
-        # self.all_true_answers: List = []
-        self.batch_size: List = []
-        self.n_batches = 0
-
-    def _add_decoded_answer_batch_stats(self, m_input, preds, **contexts):
-        # For BERT, doc_tokens = concatenated tokens from question and document.
-        doc_tokens = m_input[0]
-        starts, ends, has_answers = preds
-        pred_answers = [
-            self._unnumberize(tokens[start : end + 1].tolist(), doc_str)
-            for tokens, start, end, doc_str in zip(
-                doc_tokens, starts, ends, contexts[self.DOC_COLUMN]
-            )
-        ]
-        self.aggregate_data(self.all_pred_answers, pred_answers)
-
-    def add_batch_stats(
-        self, n_batches, preds, targets, scores, loss, m_input, **contexts
-    ):  # contexts object is the dict returned by self.batch_context().
-        super().add_batch_stats(
-            n_batches, preds, targets, scores, loss, m_input, **contexts
-        )
-        self._add_decoded_answer_batch_stats(m_input, preds, **contexts)
-
-    def aggregate_preds(self, new_batch):
-        self.aggregate_data(self.all_start_pos_preds, new_batch[0])
-        self.aggregate_data(self.all_end_pos_preds, new_batch[1])
-        self.aggregate_data(self.all_has_answer_preds, new_batch[2])
-
-    def aggregate_targets(self, new_batch):
-        self.aggregate_data(self.all_start_pos_targets, new_batch[0])
-        self.aggregate_data(self.all_end_pos_targets, new_batch[1])
-        self.aggregate_data(self.all_has_answer_targets, new_batch[2])
-
-    def aggregate_scores(self, new_batch):
-        self.aggregate_data(self.all_start_pos_scores, new_batch[0])
-        self.aggregate_data(self.all_end_pos_scores, new_batch[1])
-        self.aggregate_data(self.all_has_answer_scores, new_batch[2])
-
-    def batch_context(self, raw_batch, batch):
-        context = super().batch_context(raw_batch, batch)
-        context[self.ROW_INDEX] = [row[self.ROW_INDEX] for row in raw_batch]
-        context[self.QUES_COLUMN] = [row[self.QUES_COLUMN] for row in raw_batch]
-        context[self.ANSWERS_COLUMN] = [row[self.ANSWERS_COLUMN] for row in raw_batch]
-        context[self.DOC_COLUMN] = [row[self.DOC_COLUMN] for row in raw_batch]
-        return context
-
-    def calculate_metric(self):
-        exact_matches, count = self._compute_exact_matches(
-            self.all_pred_answers,
-            self.all_context[self.ANSWERS_COLUMN],
-            self.all_has_answer_preds,
-            self.all_has_answer_targets,
-        )
-        f1_score = self._compute_f1_score(
-            self.all_pred_answers,
-            self.all_context[self.ANSWERS_COLUMN],
-            self.all_has_answer_preds,
-            self.all_has_answer_targets,
-        )
-        self.all_preds = (
-            self.all_pred_answers,
-            self.all_start_pos_preds,
-            self.all_end_pos_preds,
-            self.all_has_answer_preds,
-        )
-        self.all_targets = (
-            self.all_context[self.ANSWERS_COLUMN],
-            self.all_start_pos_targets,
-            self.all_end_pos_targets,
-            self.all_has_answer_targets,
-        )
-        self.all_scores = (
-            self.all_start_pos_scores,
-            self.all_end_pos_scores,
-            self.all_has_answer_scores,
-        )
-        metrics = SquadMetrics(
-            exact_matches=100.0 * exact_matches / count,
-            f1_score=f1_score,
-            num_examples=count,
-        )
-        return metrics
-
-    def get_model_select_metric(self, metric: SquadMetrics):
-        return metric.f1_score
-
-    def _compute_exact_matches(
-        self,
-        pred_answer_list,
-        target_answers_list,
-        pred_has_answer_list,
-        target_has_answer_list,
-    ):
-        exact_matches = 0
-        for pred_answer, target_answers, pred_has_answer, target_has_answer in zip(
-            pred_answer_list,
-            target_answers_list,
-            pred_has_answer_list,
-            target_has_answer_list,
-        ):
-            if not self.ignore_impossible:
-                if pred_has_answer != target_has_answer:
-                    continue
-                if pred_has_answer == self.false_idx:
-                    exact_matches += 1
-                    continue
-            pred = self._normalize_answer(pred_answer)
-            for answer in target_answers:
-                true = self._normalize_answer(answer)
-                if pred == true:
-                    exact_matches += 1
-                    break
-        return exact_matches, len(pred_answer_list)
-
-    def _compute_f1_score(
-        self,
-        pred_answer_list,
-        target_answers_list,
-        pred_has_answer_list,
-        target_has_answer_list,
-    ):
-        f1_scores_sum = 0.0
-        for pred_answer, target_answers, pred_has_answer, target_has_answer in zip(
-            pred_answer_list,
-            target_answers_list,
-            pred_has_answer_list,
-            target_has_answer_list,
-        ):
-            if not self.ignore_impossible:
-                if pred_has_answer != target_has_answer:
-                    continue
-                if pred_has_answer == self.false_idx:
-                    f1_scores_sum += 1.0
-                    continue
-            f1_scores_sum += max(
-                self._compute_f1_per_answer(answer, pred_answer)
-                for answer in target_answers
-            )
-        return 100.0 * f1_scores_sum / len(pred_answer_list)
-
-    def _unnumberize(self, ans_tokens, doc_str):
-        """
-        Tokens is the span of token ids that the model predicted. We re-tokenize
-        and re-numberize the raw context (doc_str) here to get doc_tokens to get
-        access to start_idx and end_idx mappings.  At this point, ans_tokens is
-        a sub-list of doc_tokens (hopefully, if the model predicted a span in
-        the context). Then we find tokens inside doc_tokens, and return the
-        corresponding span in the raw text using the idx mapping.
-        """
-        # start_idx and end_idx are lists of char start and end positions in doc_str.
-        doc_tokens, start_idx, end_idx = self.tensorizer._lookup_tokens(doc_str)
-        doc_tokens = list(doc_tokens)
-        num_ans_tokens = len(ans_tokens)
-        answer_str = ""
-        for doc_token_idx in range(len(doc_tokens) - num_ans_tokens):
-            if doc_tokens[doc_token_idx : doc_token_idx + num_ans_tokens] == ans_tokens:
-                start_char_idx = start_idx[doc_token_idx]
-                end_char_idx = end_idx[doc_token_idx + num_ans_tokens - 1]
-                answer_str = doc_str[start_char_idx:end_char_idx]
-                break
-        return answer_str
-
-    # The following three functions are copied from Squad's evaluation script.
-    # https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/
-
-    def _normalize_answer(self, s):
-        """Lower text and remove punctuation, articles and extra whitespace."""
-
-        def white_space_fix(text):
-            return " ".join(text.split())
-
-        def remove_articles(text):
-            regex = re.compile(r"\b(a|an|the)\b", re.UNICODE)
-            return re.sub(regex, " ", text)
-
-        def remove_punc(text):
-            exclude = set(string.punctuation)
-            return "".join(ch for ch in text if ch not in exclude)
-
-        def lower(text):
-            return text.lower()
-
-        return white_space_fix(remove_articles(remove_punc(lower(s))))
-
-    def _get_tokens(self, s):
-        if not s:
-            return []
-        return self._normalize_answer(s).split()
-
-    def _compute_f1_per_answer(self, a_gold, a_pred):
-        gold_toks = self._get_tokens(a_gold)
-        pred_toks = self._get_tokens(a_pred)
-        common = Counter(gold_toks) & Counter(pred_toks)
-        num_same = sum(common.values())
-        if len(gold_toks) == 0 or len(pred_toks) == 0:
-            # If either is no-answer, then F1 is 1 if they agree, 0 otherwise
-            return int(gold_toks == pred_toks)
-        if num_same == 0:
-            return 0
-        precision = 1.0 * num_same / len(pred_toks)
-        recall = 1.0 * num_same / len(gold_toks)
-        f1 = (2 * precision * recall) / (precision + recall)
-        return f1
diff --git a/pytext/metrics/squad_metrics.py b/pytext/metrics/squad_metrics.py
deleted file mode 100644
index d4d9ee4..0000000
--- a/pytext/metrics/squad_metrics.py
+++ /dev/null
@@ -1,15 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
-
-from typing import NamedTuple
-
-
-class SquadMetrics(NamedTuple):
-    num_examples: int = 1
-    exact_matches: float = -1.0
-    f1_score: float = -1.0
-
-    def print_metrics(self) -> None:
-        print(f"Number of samples = {self.num_examples}")
-        print(f"Percentage of exact matches = {self.exact_matches}")
-        print(f"F1 score = {self.f1_score}")
diff --git a/pytext/models/output_layers/squad_output_layer.py b/pytext/models/output_layers/squad_output_layer.py
deleted file mode 100644
index fb763ec..0000000
--- a/pytext/models/output_layers/squad_output_layer.py
+++ /dev/null
@@ -1,171 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
-
-from typing import Any, Dict, Iterable, List, Optional, Tuple
-
-import torch
-import torch.nn.functional as F
-from pytext.config.component import create_loss
-from pytext.fields import FieldMeta
-from pytext.loss import CrossEntropyLoss, Loss
-from pytext.models.output_layers import OutputLayerBase
-
-
-class SquadOutputLayer(OutputLayerBase):
-    class Config(OutputLayerBase.Config):
-        loss: CrossEntropyLoss.Config = CrossEntropyLoss.Config()
-        ignore_impossible: bool = True
-        pos_loss_weight: float = 0.5
-        has_answer_loss_weight: float = 0.5
-        false_label: str = "False"
-
-    @classmethod
-    def from_config(
-        cls,
-        config,
-        metadata: Optional[FieldMeta] = None,
-        labels: Optional[Iterable[str]] = None,
-    ):
-        return cls(
-            loss_fn=create_loss(config.loss, ignore_index=-100),
-            ignore_impossible=config.ignore_impossible,
-            pos_loss_weight=config.pos_loss_weight,
-            has_answer_loss_weight=config.has_answer_loss_weight,
-            has_answer_labels=labels,
-            false_label=config.false_label,
-        )
-
-    def __init__(
-        self,
-        loss_fn: Loss,
-        ignore_impossible: bool = Config.ignore_impossible,
-        pos_loss_weight: float = Config.pos_loss_weight,
-        has_answer_loss_weight: float = Config.has_answer_loss_weight,
-        has_answer_labels: Iterable[str] = ("False", "True"),
-        false_label: str = Config.false_label,
-    ) -> None:
-        super().__init__(loss_fn=loss_fn)
-        self.pos_loss_weight = pos_loss_weight
-        self.has_answer_loss_weight = has_answer_loss_weight
-        self.has_answer_labels = has_answer_labels
-        self.ignore_impossible = ignore_impossible
-        if not ignore_impossible:
-            self.false_idx = 1 if has_answer_labels[1] == false_label else 0
-            self.true_idx = 1 - self.false_idx
-
-    def _get_position_preds(
-        self, start_pos_logits: torch.Tensor, end_pos_logits: torch.Tensor
-    ):
-        # the following is to enforce end_pos > start_pos.  We create a matrix
-        # of start_positions X end_positions, fill it with the sum logits,
-        # then mask it to be upper-triangular
-        # e.g. start_pos_logits = [1, 3, 0, 5, 2]
-        #      end_pos_logits = [2, 4, 6, 3, 5]
-        # The max indices should be (3,4) with values (5,5).  (5,6) would have a
-        # higher score, but end_pos would be before start, so it's not feasible
-        #
-        # To calculate this, first create a matrix with i,j entry containing
-        # start_pos_logits[i] + end_pos_logits[j]
-        #                   = [[3, 5, 7, 4, 6],
-        #                     [4, 7, 9, 6, 8],
-        #                     [2, 4, 6, 3, 5],
-        #                     [7, 9, 11, 8, 10],
-        #                     [4, 6, 8, 5, 7]]
-        # Then mask it to be upper-triagular:
-        #  logit_sum_matrix = [[3, 5, 7, 4, 6],
-        #                     [0, 7, 9, 6, 8],
-        #                     [0, 0, 6, 3, 5],
-        #                     [0, 0, 0, 8, 10],
-        #                     [0, 0, 0, 0, 7]]
-        # Then we use argmax to retrieve the indices of the max value.
-        size = start_pos_logits.size() + (start_pos_logits.size()[-1],)
-        start_pos_logits = start_pos_logits.unsqueeze(-1).expand(size) + 10
-        end_pos_logits = (
-            end_pos_logits.unsqueeze(-1).expand(size).transpose(-2, -1) + 10
-        )
-        logit_sum_matrix = (start_pos_logits + end_pos_logits).triu()
-        vals, ids = logit_sum_matrix.max(-1)
-        _, start_positions = vals.max(-1)
-        end_positions = ids.gather(-1, start_positions.unsqueeze(-1)).squeeze(-1)
-
-        return start_positions, end_positions
-
-    def get_pred(
-        self,
-        logits: torch.Tensor,
-        targets: torch.Tensor,
-        contexts: Dict[str, List[Any]],
-    ) -> Tuple[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:
-        start_pos_logits, end_pos_logits, has_answer_logits = logits
-        start_pos_preds, end_pos_preds = self._get_position_preds(
-            start_pos_logits, end_pos_logits
-        )
-        has_answer_preds = has_answer_logits.argmax(-1)
-        has_answer_scores = torch.zeros(has_answer_logits.size())
-        if not self.ignore_impossible:
-            has_answer_scores = F.softmax(has_answer_logits, 1)
-
-        # Compute the logit of the corresponding to start and end positions.
-        start_pos_scores = (
-            F.softmax(start_pos_logits, 1)
-            .gather(1, start_pos_preds.view(-1, 1))
-            .squeeze(-1)
-        )
-        end_pos_scores = (
-            F.softmax(end_pos_logits, 1)
-            .gather(1, end_pos_preds.view(-1, 1))
-            .squeeze(-1)
-        )
-
-        return (
-            (start_pos_preds, end_pos_preds, has_answer_preds),
-            (start_pos_scores, end_pos_scores, has_answer_scores),
-        )
-
-    def get_loss(
-        self,
-        logits: Tuple[torch.Tensor, torch.Tensor],
-        targets: Tuple[torch.Tensor, torch.Tensor, torch.Tensor],
-        contexts: Dict[str, Any] = None,
-        *args,
-        **kwargs,
-    ) -> torch.Tensor:
-        """Compute and return the loss given logits and targets.
-
-        Args:
-            logit (torch.Tensor): Logits returned :class:`~pytext.models.Model`.
-            target (torch.Tensor): True label/target to compute loss against.
-            context (Optional[Dict[str, Any]]): Context is a dictionary of items
-                that's passed as additional metadata by the
-                :class:`~pytext.data.DataHandler`. Defaults to None.=
-
-        Returns:
-            torch.Tensor: Model loss.
-
-        """
-        start_pos_logit, end_pos_logit, has_answer_logit = logits
-        start_pos_target, end_pos_target, has_answer_target = targets
-
-        num_answers = start_pos_target.size()[-1]
-        start_loss = self.loss_fn(
-            start_pos_logit.repeat((num_answers, 1)),
-            start_pos_target.transpose(1, 0).flatten(),
-            reduce=False,
-        )
-        end_loss = self.loss_fn(
-            end_pos_logit.repeat((num_answers, 1)),
-            end_pos_target.transpose(1, 0).flatten(),
-            reduce=False,
-        )
-        loss = (start_loss + end_loss).mean()
-        if not self.ignore_impossible:
-            has_answer_mask = (
-                has_answer_target.repeat((num_answers,)) == self.true_idx
-            ).float()
-            position_loss = (has_answer_mask * (start_loss + end_loss)).mean()
-            has_answer_loss = self.loss_fn(has_answer_logit, has_answer_target)
-            loss = (
-                self.has_answer_loss_weight * has_answer_loss
-                + self.pos_loss_weight * position_loss
-            )
-        return loss
diff --git a/pytext/models/qna/bert_squad_qa.py b/pytext/models/qna/bert_squad_qa.py
deleted file mode 100644
index 77c110a..0000000
--- a/pytext/models/qna/bert_squad_qa.py
+++ /dev/null
@@ -1,107 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
-import torch
-from pytext.common.constants import Stage
-from pytext.data.squad_for_bert_tensorizer import SquadForBERTTensorizer
-from pytext.data.tensorizers import LabelTensorizer
-from pytext.data.utils import Vocabulary
-from pytext.models.bert_classification_models import NewBertModel
-from pytext.models.decoders.mlp_decoder import MLPDecoder
-from pytext.models.model import BaseModel
-from pytext.models.module import create_module
-from pytext.models.output_layers.squad_output_layer import SquadOutputLayer
-from pytext.models.representations.huggingface_bert_sentence_encoder import (
-    HuggingFaceBertSentenceEncoder,
-)
-from pytext.models.representations.transformer_sentence_encoder_base import (
-    TransformerSentenceEncoderBase,
-)
-
-
-class BertSquadQAModel(NewBertModel):
-    class Config(NewBertModel.Config):
-        class ModelInput(BaseModel.Config.ModelInput):
-            squad_input: SquadForBERTTensorizer.Config = SquadForBERTTensorizer.Config(
-                max_seq_len=256
-            )
-            # is_impossible label
-            has_answer: LabelTensorizer.Config = LabelTensorizer.Config(
-                column="has_answer"
-            )
-
-        inputs: ModelInput = ModelInput()
-        encoder: TransformerSentenceEncoderBase.Config = HuggingFaceBertSentenceEncoder.Config()
-        decoder: MLPDecoder.Config = MLPDecoder.Config(out_dim=2)
-        output_layer: SquadOutputLayer.Config = SquadOutputLayer.Config()
-
-    @classmethod
-    def from_config(cls, config: Config, tensorizers):
-        has_answer_labels = ["False", "True"]
-        tensorizers["has_answer"].vocab = Vocabulary(has_answer_labels)
-        vocab = tensorizers["squad_input"].vocab
-        encoder = create_module(
-            config.encoder,
-            output_encoded_layers=True,
-            padding_idx=vocab.get_pad_index(),
-            vocab_size=vocab.__len__(),
-        )
-        decoder = create_module(
-            config.decoder, in_dim=encoder.representation_dim, out_dim=2
-        )
-        has_ans_decoder = create_module(
-            config.decoder,
-            in_dim=encoder.representation_dim,
-            out_dim=len(has_answer_labels),
-        )
-        output_layer = create_module(config.output_layer, labels=has_answer_labels)
-        return cls(encoder, decoder, has_ans_decoder, output_layer)
-
-    def __init__(
-        self, encoder, decoder, has_ans_decoder, output_layer, stage=Stage.TRAIN
-    ) -> None:
-        super().__init__(encoder, decoder, output_layer, stage)
-        self.has_ans_decoder = has_ans_decoder
-        self.module_list.append(has_ans_decoder)
-
-    def arrange_model_inputs(self, tensor_dict):
-        (
-            tokens,
-            pad_mask,
-            segment_labels,
-            answer_start_indices,
-            answer_end_indices,
-        ) = tensor_dict["squad_input"]
-        return tokens, pad_mask, segment_labels
-
-    def arrange_targets(self, tensor_dict):
-        (
-            tokens,
-            pad_mask,
-            segment_labels,
-            answer_start_indices,
-            answer_end_indices,
-        ) = tensor_dict["squad_input"]
-        # label = True if answer exists
-        label = tensor_dict["has_answer"]
-        return answer_start_indices, answer_end_indices, label
-
-    def forward(self, *inputs):
-        encoded_layers, cls_embed = self.encoder(inputs)
-        logits = self.decoder(encoded_layers[-1])
-        if isinstance(logits, (list, tuple)):
-            logits = logits[0]
-
-        label = (
-            torch.zeros((logits.size(0), 2))  # dummy tensor
-            if self.output_layer.ignore_impossible
-            else self.has_ans_decoder(cls_embed)
-        )
-        # Shape of logits is (batch_size, seq_len, 2)
-        start_logits, end_logits = logits.split(1, dim=-1)
-
-        # Shape of start_logits and end_logits is (batch_size, seq_len, 1)
-        # Hence, remove the last dimension and reduce them to the dimensions to
-        # (batch_size, seq_len)
-        start_logits = start_logits.squeeze(-1)
-        end_logits = end_logits.squeeze(-1)
-        return start_logits, end_logits, label
diff --git a/pytext/models/qna/dr_qa.py b/pytext/models/qna/dr_qa.py
deleted file mode 100644
index 0f777c0..0000000
--- a/pytext/models/qna/dr_qa.py
+++ /dev/null
@@ -1,216 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
-from typing import Dict, Tuple
-
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from pytext.data.squad_tensorizer import SquadTensorizer
-from pytext.data.tensorizers import LabelTensorizer, Tensorizer
-from pytext.data.utils import Vocabulary
-from pytext.models.decoders.mlp_decoder import MLPDecoder
-from pytext.models.embeddings import WordEmbedding
-from pytext.models.model import BaseModel
-from pytext.models.module import create_module
-from pytext.models.output_layers.squad_output_layer import SquadOutputLayer
-from pytext.models.representations.attention import (
-    DotProductSelfAttention,
-    MultiplicativeAttention,
-    SequenceAlignedAttention,
-)
-from pytext.models.representations.pooling import SelfAttention
-from pytext.models.representations.stacked_bidirectional_rnn import (
-    StackedBidirectionalRNN,
-)
-
-
-GLOVE_840B_300D = "/mnt/vol/pytext/users/kushall/pretrained/glove.840B.300d.txt"
-
-
-class DrQAModel(BaseModel):
-    class Config(BaseModel.Config):
-        class ModelInput(BaseModel.Config.ModelInput):
-            squad_input: SquadTensorizer.Config = SquadTensorizer.Config()
-            has_answer: LabelTensorizer.Config = LabelTensorizer.Config(
-                column="has_answer"
-            )
-
-        # Model inputs.
-        inputs: ModelInput = ModelInput()
-
-        # Configrable modules for the model.
-        dropout: float = 0.4  # Overrides dropout in sub-modules of the model.
-        embedding: WordEmbedding.Config = WordEmbedding.Config(
-            embed_dim=300,
-            pretrained_embeddings_path=GLOVE_840B_300D,
-            vocab_from_pretrained_embeddings=True,
-        )
-        ques_rnn: StackedBidirectionalRNN.Config = StackedBidirectionalRNN.Config(
-            dropout=dropout
-        )
-        doc_rnn: StackedBidirectionalRNN.Config = StackedBidirectionalRNN.Config(
-            dropout=dropout
-        )
-
-        # Output layer.
-        output_layer: SquadOutputLayer.Config = SquadOutputLayer.Config()
-
-    @classmethod
-    def from_config(cls, config: Config, tensorizers: Dict[str, Tensorizer]):
-        # Although the RNN params are configurable, for DrQA we want to set
-        # the following parameters for all cases.
-        config.ques_rnn.dropout = config.dropout
-        config.doc_rnn.dropout = config.dropout
-
-        embedding = cls.create_embedding(config, tensorizers)
-        ques_aligned_doc_attn = SequenceAlignedAttention(embedding.embedding_dim)
-        ques_rnn = create_module(config.ques_rnn, input_size=embedding.embedding_dim)
-        doc_rnn = create_module(config.doc_rnn, input_size=embedding.embedding_dim * 2)
-        ques_self_attn = DotProductSelfAttention(ques_rnn.representation_dim)
-        start_attn = MultiplicativeAttention(
-            doc_rnn.representation_dim, ques_rnn.representation_dim, normalize=False
-        )
-        end_attn = MultiplicativeAttention(
-            doc_rnn.representation_dim, ques_rnn.representation_dim, normalize=False
-        )
-        doc_rep_pool = SelfAttention(
-            SelfAttention.Config(dropout=config.dropout),
-            n_input=doc_rnn.representation_dim,
-        )
-        has_answer_labels = ["False", "True"]
-        tensorizers["has_answer"].vocab = Vocabulary(has_answer_labels)
-        has_ans_decoder = MLPDecoder(
-            config=MLPDecoder.Config(),
-            in_dim=doc_rnn.representation_dim,
-            out_dim=len(has_answer_labels),
-        )
-        output_layer = create_module(config.output_layer, labels=has_answer_labels)
-        return cls(
-            dropout=nn.Dropout(config.dropout),
-            embedding=embedding,
-            ques_rnn=ques_rnn,
-            doc_rnn=doc_rnn,
-            ques_self_attn=ques_self_attn,
-            ques_aligned_doc_attn=ques_aligned_doc_attn,
-            start_attn=start_attn,
-            end_attn=end_attn,
-            doc_rep_pool=doc_rep_pool,
-            has_ans_decoder=has_ans_decoder,
-            output_layer=output_layer,
-        )
-
-    @classmethod
-    def create_embedding(cls, model_config: Config, tensorizers: Dict[str, Tensorizer]):
-        squad_tensorizer = tensorizers["squad_input"]
-
-        # Initialize the embedding module.
-        embedding_module = create_module(model_config.embedding, None, squad_tensorizer)
-
-        # Set ques and doc tensorizer vocab to squad_tensorizer.vocab.
-        squad_tensorizer.ques_tensorizer.vocab = squad_tensorizer.vocab
-        squad_tensorizer.doc_tensorizer.vocab = squad_tensorizer.vocab
-
-        return embedding_module
-
-    def __init__(
-        self,
-        dropout,
-        embedding,
-        ques_rnn,
-        doc_rnn,
-        ques_self_attn,
-        ques_aligned_doc_attn,
-        start_attn,
-        end_attn,
-        doc_rep_pool,
-        has_ans_decoder,
-        output_layer,
-    ) -> None:
-        super().__init__()
-        self.dropout = dropout
-        self.embedding = embedding
-        self.ques_rnn = ques_rnn
-        self.doc_rnn = doc_rnn
-        self.ques_self_attn = ques_self_attn
-        self.ques_aligned_doc_attn = ques_aligned_doc_attn
-        self.start_attn = start_attn
-        self.end_attn = end_attn
-        self.doc_rep_pool = doc_rep_pool
-        self.has_ans_decoder = has_ans_decoder
-        self.output_layer = output_layer
-        self.ignore_impossible = output_layer.ignore_impossible
-        self.module_list = [
-            embedding,
-            ques_rnn,
-            doc_rnn,
-            ques_self_attn,
-            ques_aligned_doc_attn,
-            start_attn,
-            end_attn,
-            has_ans_decoder,
-        ]
-
-    def arrange_model_inputs(self, tensor_dict):
-        (
-            doc_tokens,
-            doc_seq_len,
-            doc_mask,
-            ques_tokens,
-            ques_seq_len,
-            ques_mask,
-            _,
-            _,
-        ) = tensor_dict["squad_input"]
-        return (doc_tokens, doc_seq_len, doc_mask, ques_tokens, ques_seq_len, ques_mask)
-
-    def arrange_targets(self, tensor_dict):
-        _, _, _, _, _, _, answer_start_idx, answer_end_idx = tensor_dict["squad_input"]
-        has_answer = tensor_dict["has_answer"]
-        return answer_start_idx, answer_end_idx, has_answer
-
-    def forward(
-        self,
-        doc_tokens: torch.Tensor,
-        doc_seq_len: torch.Tensor,
-        doc_mask: torch.Tensor,
-        ques_tokens: torch.Tensor,
-        ques_seq_len: torch.Tensor,
-        ques_mask: torch.Tensor,
-    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
-        # Embedding lookups.
-        doc_embedded = self.dropout(self.embedding(doc_tokens))
-        ques_embedded = self.dropout(self.embedding(ques_tokens))
-
-        # Document encoding.
-        doc_attn_weights = self.ques_aligned_doc_attn(
-            doc_embedded, ques_embedded, ques_mask
-        )
-        ques_aligned_doc_embedded = doc_attn_weights.bmm(ques_embedded)
-        doc_embedded = torch.cat([doc_embedded, ques_aligned_doc_embedded], dim=2)
-        doc_seq_vec = self.doc_rnn(doc_embedded, doc_mask)
-
-        # Question encoding.
-        ques_seq_vec = self.ques_rnn(ques_embedded, ques_mask)
-        ques_attn_weights = self.ques_self_attn(ques_seq_vec, ques_mask)
-        ques_vec = ques_attn_weights.unsqueeze(1).bmm(ques_seq_vec).squeeze(1)
-
-        # Apply dropout to the ques and document representations.
-        doc_seq_vec = self.dropout(doc_seq_vec)
-        ques_vec = self.dropout(ques_vec)
-
-        # Compute bilinear attention weights for each document token.
-        # These attention weights serve as logits for detecting start and end.
-        start_logits = self.start_attn(doc_seq_vec, ques_vec, doc_mask)
-        end_logits = self.end_attn(doc_seq_vec, ques_vec, doc_mask)
-
-        has_answer_logits = torch.zeros(
-            ques_tokens.size(0),  # batch_size
-            self.has_ans_decoder.out_dim,  # Number of classes
-        )
-        if not self.ignore_impossible:  # Compute whether document has an answer.
-            doc_vec = self.doc_rep_pool(doc_seq_vec, doc_seq_len)
-            has_answer_logits = F.relu(self.has_ans_decoder(doc_vec))
-
-        # start_logits and end_logits: batch_size, max_seq_len
-        # has_answer_logit: batch_size, 2
-        return start_logits, end_logits, has_answer_logits
diff --git a/pytext/models/test/transformer_sentence_encoder_test.py b/pytext/models/test/transformer_sentence_encoder_test.py
deleted file mode 100644
index 8dad498..0000000
--- a/pytext/models/test/transformer_sentence_encoder_test.py
+++ /dev/null
@@ -1,87 +0,0 @@
-#!/usr/bin/env python3
-# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
-import unittest
-
-import torch
-from pytext.models.representations.transformer_sentence_encoder import (
-    TransformerSentenceEncoder,
-)
-
-
-class TransformerSentenceEncoderTest(unittest.TestCase):
-    def setUp(self):
-        self.batch_size = 10
-        self.num_tokens = 20
-        self.embedding_dim = 1024
-        self.vocab_size = 1000
-        self.padding_idx = 0
-        self.num_encoder_layers = 6
-
-        # Generate a tensor of token ids as input tokens
-        self.tokens = (
-            torch.randint(5, 1000, (self.batch_size, self.num_tokens))
-        ).long()
-        self.lengths = torch.tensor([self.num_tokens])
-        self.pad_mask = (torch.ones(self.batch_size, self.num_tokens)).long()
-        self.segment_labels = (torch.ones(self.batch_size, self.num_tokens)).long()
-        self.positions = None
-
-    def test_monolingual_transformer_sentence_encoder(self):
-
-        input_tuple = (self.tokens, self.pad_mask, self.segment_labels)
-
-        sentence_encoder = TransformerSentenceEncoder.from_config(
-            TransformerSentenceEncoder.Config(
-                embedding_dim=self.embedding_dim,
-                num_encoder_layers=self.num_encoder_layers,
-                multilingual=False,
-            ),
-            output_encoded_layers=True,
-            padding_idx=self.padding_idx,
-            vocab_size=self.vocab_size,
-        )
-
-        encoded_layers, pooled_outputs = sentence_encoder(input_tuple)
-
-        # Check sizes for pooled output
-        self.assertEqual(pooled_outputs.size()[0], self.batch_size)
-        self.assertEqual(pooled_outputs.size()[1], self.embedding_dim)
-
-        # Check sizes for encoded_layers
-        self.assertEqual(encoded_layers.__len__(), self.num_encoder_layers + 1)
-        self.assertEqual(encoded_layers[-1].size()[0], self.batch_size)
-        self.assertEqual(encoded_layers[-1].size()[1], self.num_tokens)
-        self.assertEqual(encoded_layers[-1].size()[2], self.embedding_dim)
-
-    def test_multilingual_transformer_sentence_encoder(self):
-
-        input_tuple = (
-            self.tokens,
-            self.pad_mask,
-            self.lengths,
-            self.segment_labels,
-            self.positions,
-        )
-
-        sentence_encoder = TransformerSentenceEncoder.from_config(
-            TransformerSentenceEncoder.Config(
-                embedding_dim=self.embedding_dim,
-                num_encoder_layers=self.num_encoder_layers,
-                multilingual=True,
-            ),
-            output_encoded_layers=True,
-            padding_idx=self.padding_idx,
-            vocab_size=self.vocab_size,
-        )
-
-        encoded_layers, pooled_outputs = sentence_encoder(input_tuple)
-
-        # Check sizes for pooled output
-        self.assertEqual(pooled_outputs.size()[0], self.batch_size)
-        self.assertEqual(pooled_outputs.size()[1], self.embedding_dim)
-
-        # Check sizes for encoded_layers
-        self.assertEqual(encoded_layers.__len__(), self.num_encoder_layers + 1)
-        self.assertEqual(encoded_layers[-1].size()[0], self.batch_size)
-        self.assertEqual(encoded_layers[-1].size()[1], self.num_tokens)
-        self.assertEqual(encoded_layers[-1].size()[2], self.embedding_dim)
diff --git a/pytext/task/tasks.py b/pytext/task/tasks.py
index 4b8df8c..741187c 100644
--- a/pytext/task/tasks.py
+++ b/pytext/task/tasks.py
@@ -29,7 +29,6 @@ from pytext.metric_reporters import (
     PairwiseRankingMetricReporter,
     RegressionMetricReporter,
     SequenceTaggingMetricReporter,
-    SquadMetricReporter,
     WordTaggingMetricReporter,
 )
 from pytext.models.doc_model import DocModel, DocModel_Deprecated, DocRegressionModel
@@ -38,8 +37,6 @@ from pytext.models.joint_model import IntentSlotModel
 from pytext.models.language_models.lmlstm import LMLSTM, LMLSTM_Deprecated
 from pytext.models.model import BaseModel
 from pytext.models.pair_classification_model import BasePairwiseModel, PairwiseModel
-from pytext.models.qna.bert_squad_qa import BertSquadQAModel
-from pytext.models.qna.dr_qa import DrQAModel
 from pytext.models.query_document_pairwise_ranking_model import (
     QueryDocPairwiseRankingModel,
     QueryDocumentPairwiseRankingModel_Deprecated,
@@ -266,12 +263,6 @@ class SeqNNTask(NewTask):
         )
 
 
-class SquadQATask(NewTask):
-    class Config(NewTask.Config):
-        model: Union[BertSquadQAModel.Config, DrQAModel.Config] = DrQAModel.Config()
-        metric_reporter: SquadMetricReporter.Config = SquadMetricReporter.Config()
-
-
 class SemanticParsingTask_Deprecated(Task_Deprecated):
     class Config(Task_Deprecated.Config):
         model: RNNGParser_Deprecated.Config = RNNGParser_Deprecated.Config()
-- 
1.8.3.1

